{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'> **HW4 Dataset 4**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n",
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "Loading required package: Matrix\n",
      "Loading required package: foreach\n",
      "Loaded glmnet 2.0-16\n",
      "\n",
      "Warning message:\n",
      "\"package 'CVXR' was built under R version 3.6.3\"\n",
      "Attaching package: 'CVXR'\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    power\n",
      "\n",
      "Warning message:\n",
      "\"package 'MLmetrics' was built under R version 3.6.3\"\n",
      "Attaching package: 'MLmetrics'\n",
      "\n",
      "The following objects are masked from 'package:caret':\n",
      "\n",
      "    MAE, RMSE\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    Recall\n",
      "\n",
      "Warning message:\n",
      "\"package 'gbm' was built under R version 3.6.3\"Loaded gbm 2.1.8\n",
      "Registered S3 method overwritten by 'rvest':\n",
      "  method            from\n",
      "  read_xml.response xml2\n",
      "-- Attaching packages --------------------------------------- tidyverse 1.2.1 --\n",
      "<U+221A> tibble  2.1.1       <U+221A> purrr   0.3.2  \n",
      "<U+221A> tidyr   0.8.3       <U+221A> dplyr   0.8.0.1\n",
      "<U+221A> readr   1.3.1       <U+221A> stringr 1.4.0  \n",
      "<U+221A> tibble  2.1.1       <U+221A> forcats 0.4.0  \n",
      "-- Conflicts ------------------------------------------ tidyverse_conflicts() --\n",
      "x purrr::accumulate() masks foreach::accumulate()\n",
      "x dplyr::between()    masks data.table::between()\n",
      "x tidyr::expand()     masks Matrix::expand()\n",
      "x dplyr::filter()     masks stats::filter()\n",
      "x dplyr::first()      masks data.table::first()\n",
      "x dplyr::id()         masks CVXR::id()\n",
      "x purrr::is_vector()  masks CVXR::is_vector()\n",
      "x dplyr::lag()        masks stats::lag()\n",
      "x dplyr::last()       masks data.table::last()\n",
      "x purrr::lift()       masks caret::lift()\n",
      "x purrr::transpose()  masks data.table::transpose()\n",
      "x purrr::when()       masks foreach::when()\n"
     ]
    }
   ],
   "source": [
    "#install.packages(\"MLmetrics\")\n",
    "#install.packages(\"gbm\")\n",
    "library(caret)\n",
    "library(glmnet)\n",
    "library(CVXR)\n",
    "library(MLmetrics)\n",
    "library(data.table)\n",
    "library(glmnet)\n",
    "library(gbm)\n",
    "library(tidyverse)\n",
    "library(readxl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> **Dataset 4: Spambase Data Set**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Title:  SPAM E-mail Database\n",
    "\n",
    "2. Sources:\n",
    "   (a) Creators: Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt\n",
    "        Hewlett-Packard Labs, 1501 Page Mill Rd., Palo Alto, CA 94304\n",
    "   (b) Donor: George Forman (gforman at nospam hpl.hp.com)  650-857-7835\n",
    "   (c) Generated: June-July 1999\n",
    "\n",
    "3. Past Usage:\n",
    "   (a) Hewlett-Packard Internal-only Technical Report. External forthcoming.\n",
    "   (b) Determine whether a given email is spam or not.\n",
    "   (c) ~7% misclassification error.\n",
    "       False positives (marking good mail as spam) are very undesirable.\n",
    "       If we insist on zero false positives in the training/testing set,\n",
    "       20-25% of the spam passed through the filter.\n",
    "\n",
    "4. Relevant Information:\n",
    "        The \"spam\" concept is diverse: advertisements for products/web\n",
    "        sites, make money fast schemes, chain letters, pornography...\n",
    "\tOur collection of spam e-mails came from our postmaster and \n",
    "\tindividuals who had filed spam.  Our collection of non-spam \n",
    "\te-mails came from filed work and personal e-mails, and hence\n",
    "\tthe word 'george' and the area code '650' are indicators of \n",
    "\tnon-spam.  These are useful when constructing a personalized \n",
    "\tspam filter.  One would either have to blind such non-spam \n",
    "\tindicators or get a very wide collection of non-spam to \n",
    "\tgenerate a general purpose spam filter.\n",
    "\n",
    "        For background on spam:\n",
    "        Cranor, Lorrie F., LaMacchia, Brian A.  Spam! \n",
    "        Communications of the ACM, 41(8):74-83, 1998.\n",
    "\n",
    "5. Number of Instances: 4601 (1813 Spam = 39.4%)\n",
    "\n",
    "6. Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
    "\n",
    "7. Attribute Information:\n",
    "The last column of 'spambase.data' denotes whether the e-mail was \n",
    "considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \n",
    "Most of the attributes indicate whether a particular word or\n",
    "character was frequently occuring in the e-mail.  The run-length\n",
    "attributes (55-57) measure the length of sequences of consecutive \n",
    "capital letters.  For the statistical measures of each attribute, \n",
    "see the end of this file.  Here are the definitions of the attributes:\n",
    "\n",
    "48 continuous real [0,100] attributes of type word_freq_WORD \n",
    "= percentage of words in the e-mail that match WORD,\n",
    "i.e. 100 * (number of times the WORD appears in the e-mail) / \n",
    "total number of words in e-mail.  A \"word\" in this case is any \n",
    "string of alphanumeric characters bounded by non-alphanumeric \n",
    "characters or end-of-string.\n",
    "\n",
    "6 continuous real [0,100] attributes of type char_freq_CHAR\n",
    "= percentage of characters in the e-mail that match CHAR,\n",
    "i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
    "\n",
    "1 continuous real [1,...] attribute of type capital_run_length_average\n",
    "= average length of uninterrupted sequences of capital letters\n",
    "\n",
    "1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
    "= length of longest uninterrupted sequence of capital letters\n",
    "\n",
    "1 continuous integer [1,...] attribute of type capital_run_length_total\n",
    "= sum of length of uninterrupted sequences of capital letters\n",
    "= total number of capital letters in the e-mail\n",
    "\n",
    "1 nominal {0,1} class attribute of type spam\n",
    "= denotes whether the e-mail was considered spam (1) or not (0), \n",
    "i.e. unsolicited commercial e-mail.  \n",
    "\n",
    "\n",
    "8. Missing Attribute Values: None\n",
    "\n",
    "9. Class Distribution:\n",
    "\tSpam\t  1813  (39.4%)\n",
    "\tNon-Spam  2788  (60.6%)\n",
    "\n",
    "\n",
    "Attribute Statistics:\n",
    "   Min: Max:   Average:  Std.Dev: Coeff.Var_%: \n",
    "1  0    4.54   0.10455   0.30536  292          \n",
    "2  0    14.28  0.21301   1.2906   606          \n",
    "3  0    5.1    0.28066   0.50414  180          \n",
    "4  0    42.81  0.065425  1.3952   2130         \n",
    "5  0    10     0.31222   0.67251  215          \n",
    "6  0    5.88   0.095901  0.27382  286          \n",
    "7  0    7.27   0.11421   0.39144  343          \n",
    "8  0    11.11  0.10529   0.40107  381          \n",
    "9  0    5.26   0.090067  0.27862  309          \n",
    "10 0    18.18  0.23941   0.64476  269          \n",
    "11 0    2.61   0.059824  0.20154  337          \n",
    "12 0    9.67   0.5417    0.8617   159          \n",
    "13 0    5.55   0.09393   0.30104  320          \n",
    "14 0    10     0.058626  0.33518  572          \n",
    "15 0    4.41   0.049205  0.25884  526          \n",
    "16 0    20     0.24885   0.82579  332          \n",
    "17 0    7.14   0.14259   0.44406  311          \n",
    "18 0    9.09   0.18474   0.53112  287          \n",
    "19 0    18.75  1.6621    1.7755   107          \n",
    "20 0    18.18  0.085577  0.50977  596          \n",
    "21 0    11.11  0.80976   1.2008   148          \n",
    "22 0    17.1   0.1212    1.0258   846          \n",
    "23 0    5.45   0.10165   0.35029  345          \n",
    "24 0    12.5   0.094269  0.44264  470          \n",
    "25 0    20.83  0.5495    1.6713   304          \n",
    "26 0    16.66  0.26538   0.88696  334          \n",
    "27 0    33.33  0.7673    3.3673   439          \n",
    "28 0    9.09   0.12484   0.53858  431          \n",
    "29 0    14.28  0.098915  0.59333  600          \n",
    "30 0    5.88   0.10285   0.45668  444          \n",
    "31 0    12.5   0.064753  0.40339  623          \n",
    "32 0    4.76   0.047048  0.32856  698          \n",
    "33 0    18.18  0.097229  0.55591  572          \n",
    "34 0    4.76   0.047835  0.32945  689          \n",
    "35 0    20     0.10541   0.53226  505          \n",
    "36 0    7.69   0.097477  0.40262  413          \n",
    "37 0    6.89   0.13695   0.42345  309          \n",
    "38 0    8.33   0.013201  0.22065  1670         \n",
    "39 0    11.11  0.078629  0.43467  553          \n",
    "40 0    4.76   0.064834  0.34992  540          \n",
    "41 0    7.14   0.043667  0.3612   827          \n",
    "42 0    14.28  0.13234   0.76682  579          \n",
    "43 0    3.57   0.046099  0.22381  486          \n",
    "44 0    20     0.079196  0.62198  785          \n",
    "45 0    21.42  0.30122   1.0117   336          \n",
    "46 0    22.05  0.17982   0.91112  507          \n",
    "47 0    2.17   0.0054445 0.076274 1400         \n",
    "48 0    10     0.031869  0.28573  897          \n",
    "49 0    4.385  0.038575  0.24347  631          \n",
    "50 0    9.752  0.13903   0.27036  194          \n",
    "51 0    4.081  0.016976  0.10939  644          \n",
    "52 0    32.478 0.26907   0.81567  303          \n",
    "53 0    6.003  0.075811  0.24588  324          \n",
    "54 0    19.829 0.044238  0.42934  971          \n",
    "55 1    1102.5 5.1915    31.729   611          \n",
    "56 1    9989   52.173    194.89   374          \n",
    "57 1    15841  283.29    606.35   214          \n",
    "58 0    1      0.39404   0.4887   124          \n",
    "\n",
    "\n",
    "This file: 'spambase.DOCUMENTATION' at the UCI Machine Learning Repository\n",
    "http://www.ics.uci.edu/~mlearn/MLRepository.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitControl = trainControl(method = \"cv\", number = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_grid = expand.grid(alpha = 1, lambda = c(0,0.5,1))\n",
    "dt_grid = expand.grid(cp = c(0.01,0.05,0.1))\n",
    "rf_grid = expand.grid( mtry = c(3,5,7))\n",
    "sgb_grid = expand.grid(n.trees = c(30, 50,100), shrinkage = c(0.05,0.1), interaction.depth = c(10, 20), n.minobsinnode = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>V1</th><th scope=col>V2</th><th scope=col>V3</th><th scope=col>V4</th><th scope=col>V5</th><th scope=col>V6</th><th scope=col>V7</th><th scope=col>V8</th><th scope=col>V9</th><th scope=col>V10</th><th scope=col>...</th><th scope=col>V49</th><th scope=col>V50</th><th scope=col>V51</th><th scope=col>V52</th><th scope=col>V53</th><th scope=col>V54</th><th scope=col>V55</th><th scope=col>V56</th><th scope=col>V57</th><th scope=col>V58</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.00  </td><td>0.64  </td><td>0.64  </td><td>0     </td><td>0.32  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td>0.778 </td><td>0.000 </td><td>0.000 </td><td> 3.756</td><td> 61   </td><td> 278  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.21  </td><td>0.28  </td><td>0.50  </td><td>0     </td><td>0.14  </td><td>0.28  </td><td>0.21  </td><td>0.07  </td><td>0.00  </td><td>0.94  </td><td>...   </td><td>0.000 </td><td>0.132 </td><td>0.000 </td><td>0.372 </td><td>0.180 </td><td>0.048 </td><td> 5.114</td><td>101   </td><td>1028  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.06  </td><td>0.00  </td><td>0.71  </td><td>0     </td><td>1.23  </td><td>0.19  </td><td>0.19  </td><td>0.12  </td><td>0.64  </td><td>0.25  </td><td>...   </td><td>0.010 </td><td>0.143 </td><td>0.000 </td><td>0.276 </td><td>0.184 </td><td>0.010 </td><td> 9.821</td><td>485   </td><td>2259  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>0.63  </td><td>0.00  </td><td>0.31  </td><td>0.63  </td><td>0.31  </td><td>0.63  </td><td>...   </td><td>0.000 </td><td>0.137 </td><td>0.000 </td><td>0.137 </td><td>0.000 </td><td>0.000 </td><td> 3.537</td><td> 40   </td><td> 191  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>0.63  </td><td>0.00  </td><td>0.31  </td><td>0.63  </td><td>0.31  </td><td>0.63  </td><td>...   </td><td>0.000 </td><td>0.135 </td><td>0.000 </td><td>0.135 </td><td>0.000 </td><td>0.000 </td><td> 3.537</td><td> 40   </td><td> 191  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>1.85  </td><td>0.00  </td><td>0.00  </td><td>1.85  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.223 </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td> 3.000</td><td> 15   </td><td>  54  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>1.92  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.64  </td><td>...   </td><td>0.000 </td><td>0.054 </td><td>0.000 </td><td>0.164 </td><td>0.054 </td><td>0.000 </td><td> 1.671</td><td>  4   </td><td> 112  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>1.88  </td><td>0.00  </td><td>0.00  </td><td>1.88  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.206 </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td> 2.450</td><td> 11   </td><td>  49  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.15  </td><td>0.00  </td><td>0.46  </td><td>0     </td><td>0.61  </td><td>0.00  </td><td>0.30  </td><td>0.00  </td><td>0.92  </td><td>0.76  </td><td>...   </td><td>0.000 </td><td>0.271 </td><td>0.000 </td><td>0.181 </td><td>0.203 </td><td>0.022 </td><td> 9.744</td><td>445   </td><td>1257  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.06  </td><td>0.12  </td><td>0.77  </td><td>0     </td><td>0.19  </td><td>0.32  </td><td>0.38  </td><td>0.00  </td><td>0.06  </td><td>0.00  </td><td>...   </td><td>0.040 </td><td>0.030 </td><td>0.000 </td><td>0.244 </td><td>0.081 </td><td>0.000 </td><td> 1.729</td><td> 43   </td><td> 749  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>0.00  </td><td>0.00  </td><td>0.96  </td><td>0.00  </td><td>0.00  </td><td>1.92  </td><td>...   </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td>0.462 </td><td>0.000 </td><td>0.000 </td><td> 1.312</td><td>  6   </td><td>  21  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.25  </td><td>0     </td><td>0.38  </td><td>0.25  </td><td>0.25  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.022 </td><td>0.044 </td><td>0.000 </td><td>0.663 </td><td>0.000 </td><td>0.000 </td><td> 1.243</td><td> 11   </td><td> 184  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.69  </td><td>0.34  </td><td>0     </td><td>0.34  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.056 </td><td>0.000 </td><td>0.786 </td><td>0.000 </td><td>0.000 </td><td> 3.728</td><td> 61   </td><td> 261  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>0.90  </td><td>0.00  </td><td>0.90  </td><td>0.00  </td><td>0.00  </td><td>0.90  </td><td>...   </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td> 2.083</td><td>  7   </td><td>  25  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>1.42  </td><td>0     </td><td>0.71  </td><td>0.35  </td><td>0.00  </td><td>0.35  </td><td>0.00  </td><td>0.71  </td><td>...   </td><td>0.000 </td><td>0.102 </td><td>0.000 </td><td>0.357 </td><td>0.000 </td><td>0.000 </td><td> 1.971</td><td> 24   </td><td> 205  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.42  </td><td>0.42  </td><td>0     </td><td>1.27  </td><td>0.00  </td><td>0.42  </td><td>0.00  </td><td>0.00  </td><td>1.27  </td><td>...   </td><td>0.000 </td><td>0.063 </td><td>0.000 </td><td>0.572 </td><td>0.063 </td><td>0.000 </td><td> 5.659</td><td> 55   </td><td> 249  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>0.94  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td>0.428 </td><td>0.000 </td><td>0.000 </td><td> 4.652</td><td> 31   </td><td> 107  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td>1.975 </td><td>0.370 </td><td>0.000 </td><td>35.461</td><td> 95   </td><td> 461  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.55  </td><td>0     </td><td>1.11  </td><td>0.00  </td><td>0.18  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.182 </td><td>0.000 </td><td>0.455 </td><td>0.000 </td><td>0.000 </td><td> 1.320</td><td>  4   </td><td>  70  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.63  </td><td>0.00  </td><td>0     </td><td>1.59  </td><td>0.31  </td><td>0.00  </td><td>0.00  </td><td>0.31  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.275 </td><td>0.000 </td><td>0.055 </td><td>0.496 </td><td>0.000 </td><td> 3.509</td><td> 91   </td><td> 186  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.729 </td><td>0.000 </td><td>0.729 </td><td>0.000 </td><td>0.000 </td><td> 3.833</td><td>  9   </td><td>  23  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.05  </td><td>0.07  </td><td>0.10  </td><td>0     </td><td>0.76  </td><td>0.05  </td><td>0.15  </td><td>0.02  </td><td>0.55  </td><td>0.00  </td><td>...   </td><td>0.042 </td><td>0.101 </td><td>0.016 </td><td>0.250 </td><td>0.046 </td><td>0.059 </td><td> 2.569</td><td> 66   </td><td>2259  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>2.94  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.404 </td><td>0.404 </td><td>0.000 </td><td>0.809 </td><td>0.000 </td><td>0.000 </td><td> 4.857</td><td> 12   </td><td>  34  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>1.16  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.133 </td><td>0.000 </td><td>0.667 </td><td>0.000 </td><td>0.000 </td><td> 1.131</td><td>  5   </td><td>  69  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.196 </td><td>0.000 </td><td>0.392 </td><td>0.196 </td><td>0.000 </td><td> 5.466</td><td> 22   </td><td>  82  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.05  </td><td>0.07  </td><td>0.10  </td><td>0     </td><td>0.76  </td><td>0.05  </td><td>0.15  </td><td>0.02  </td><td>0.55  </td><td>0.00  </td><td>...   </td><td>0.042 </td><td>0.101 </td><td>0.016 </td><td>0.250 </td><td>0.046 </td><td>0.059 </td><td> 2.565</td><td> 66   </td><td>2258  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.196 </td><td>0.000 </td><td>0.392 </td><td>0.196 </td><td>0.000 </td><td> 5.466</td><td> 22   </td><td>  82  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>0.00  </td><td>0.00  </td><td>1.66  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.000 </td><td>0.000 </td><td>0.368 </td><td>0.000 </td><td>0.000 </td><td> 2.611</td><td> 12   </td><td>  47  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.352 </td><td>0.000 </td><td>0.352 </td><td>0.000 </td><td>0.000 </td><td> 4.000</td><td> 11   </td><td>  36  </td><td>1     </td></tr>\n",
       "\t<tr><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>0     </td><td>0.65  </td><td>0.00  </td><td>0.65  </td><td>0.00  </td><td>0.00  </td><td>0.00  </td><td>...   </td><td>0.000 </td><td>0.459 </td><td>0.000 </td><td>0.091 </td><td>0.000 </td><td>0.000 </td><td> 2.687</td><td> 66   </td><td> 129  </td><td>1     </td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>   </td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.46 </td><td>0    </td><td>0.23 </td><td>0.23 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.082</td><td>0.000</td><td>0.082</td><td>0.000</td><td>0.000</td><td>1.256</td><td> 5   </td><td>  98 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.254</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.000</td><td> 1   </td><td>  13 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.18 </td><td>0    </td><td>0.18 </td><td>0.18 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.033</td><td>0.033</td><td>0.000</td><td>0.099</td><td>0.000</td><td>0.000</td><td>1.489</td><td>11   </td><td> 137 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.29 </td><td>0.00 </td><td>0.29 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.29 </td><td>...  </td><td>0.000</td><td>0.107</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.220</td><td> 6   </td><td>  61 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>1.38 </td><td>...  </td><td>0.000</td><td>0.213</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.720</td><td>11   </td><td>  43 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.131</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.488</td><td> 5   </td><td>  64 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>1.20 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.200</td><td> 3   </td><td>  24 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.40 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.000</td><td>0.145</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.372</td><td> 5   </td><td>  70 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.27 </td><td>0.05 </td><td>0.10 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.607</td><td>0.064</td><td>0.036</td><td>0.055</td><td>0.000</td><td>0.202</td><td>3.766</td><td>43   </td><td>1789 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.571</td><td> 5   </td><td>  11 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.51 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.091</td><td>0.000</td><td>0.091</td><td>0.000</td><td>0.000</td><td>1.586</td><td> 4   </td><td>  46 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.266</td><td> 3   </td><td>  19 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>1.23 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.000</td><td>0.406</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.666</td><td>13   </td><td>  70 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.45 </td><td>0    </td><td>0.00 </td><td>0.22 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.082</td><td>0.000</td><td>0.041</td><td>0.000</td><td>0.000</td><td>1.500</td><td> 7   </td><td> 123 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.625</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.375</td><td> 4   </td><td>  11 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.36 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.112</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.056</td><td>1.793</td><td>21   </td><td> 174 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.125</td><td>0.000</td><td>0.000</td><td>0.125</td><td>0.000</td><td>1.272</td><td> 4   </td><td>  28 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>3.03 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.111</td><td> 2   </td><td>  10 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.54 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.000</td><td> 1   </td><td>  22 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.185</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.092</td><td>2.468</td><td>11   </td><td>  79 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.000</td><td> 1   </td><td>   8 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>1.25 </td><td>0    </td><td>2.50 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.111</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.285</td><td> 4   </td><td>  27 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.052</td><td>0.000</td><td>0.000</td><td>1.000</td><td> 1   </td><td>   6 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.630</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.727</td><td> 5   </td><td>  19 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>1.19 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.000</td><td> 1   </td><td>  24 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.31 </td><td>0.00 </td><td>0.62 </td><td>0    </td><td>0.00 </td><td>0.31 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.232</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.142</td><td> 3   </td><td>  88 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.353</td><td>0.000</td><td>0.000</td><td>1.555</td><td> 4   </td><td>  14 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.30 </td><td>0.00 </td><td>0.30 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.102</td><td>0.718</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.404</td><td> 6   </td><td> 118 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.96 </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0.32 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.057</td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.000</td><td>1.147</td><td> 5   </td><td>  78 </td><td>0    </td></tr>\n",
       "\t<tr><td>0.00 </td><td>0.00 </td><td>0.65 </td><td>0    </td><td>0.00 </td><td>0.00 </td><td>0    </td><td>0    </td><td>0    </td><td>0.00 </td><td>...  </td><td>0.000</td><td>0.000</td><td>0.000</td><td>0.125</td><td>0.000</td><td>0.000</td><td>1.250</td><td> 5   </td><td>  40 </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       " V1 & V2 & V3 & V4 & V5 & V6 & V7 & V8 & V9 & V10 & ... & V49 & V50 & V51 & V52 & V53 & V54 & V55 & V56 & V57 & V58\\\\\n",
       "\\hline\n",
       "\t 0.00   & 0.64   & 0.64   & 0      & 0.32   & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & ...    & 0.000  & 0.000  & 0.000  & 0.778  & 0.000  & 0.000  &  3.756 &  61    &  278   & 1     \\\\\n",
       "\t 0.21   & 0.28   & 0.50   & 0      & 0.14   & 0.28   & 0.21   & 0.07   & 0.00   & 0.94   & ...    & 0.000  & 0.132  & 0.000  & 0.372  & 0.180  & 0.048  &  5.114 & 101    & 1028   & 1     \\\\\n",
       "\t 0.06   & 0.00   & 0.71   & 0      & 1.23   & 0.19   & 0.19   & 0.12   & 0.64   & 0.25   & ...    & 0.010  & 0.143  & 0.000  & 0.276  & 0.184  & 0.010  &  9.821 & 485    & 2259   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 0.63   & 0.00   & 0.31   & 0.63   & 0.31   & 0.63   & ...    & 0.000  & 0.137  & 0.000  & 0.137  & 0.000  & 0.000  &  3.537 &  40    &  191   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 0.63   & 0.00   & 0.31   & 0.63   & 0.31   & 0.63   & ...    & 0.000  & 0.135  & 0.000  & 0.135  & 0.000  & 0.000  &  3.537 &  40    &  191   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 1.85   & 0.00   & 0.00   & 1.85   & 0.00   & 0.00   & ...    & 0.000  & 0.223  & 0.000  & 0.000  & 0.000  & 0.000  &  3.000 &  15    &   54   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 1.92   & 0.00   & 0.00   & 0.00   & 0.00   & 0.64   & ...    & 0.000  & 0.054  & 0.000  & 0.164  & 0.054  & 0.000  &  1.671 &   4    &  112   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 1.88   & 0.00   & 0.00   & 1.88   & 0.00   & 0.00   & ...    & 0.000  & 0.206  & 0.000  & 0.000  & 0.000  & 0.000  &  2.450 &  11    &   49   & 1     \\\\\n",
       "\t 0.15   & 0.00   & 0.46   & 0      & 0.61   & 0.00   & 0.30   & 0.00   & 0.92   & 0.76   & ...    & 0.000  & 0.271  & 0.000  & 0.181  & 0.203  & 0.022  &  9.744 & 445    & 1257   & 1     \\\\\n",
       "\t 0.06   & 0.12   & 0.77   & 0      & 0.19   & 0.32   & 0.38   & 0.00   & 0.06   & 0.00   & ...    & 0.040  & 0.030  & 0.000  & 0.244  & 0.081  & 0.000  &  1.729 &  43    &  749   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 0.00   & 0.00   & 0.96   & 0.00   & 0.00   & 1.92   & ...    & 0.000  & 0.000  & 0.000  & 0.462  & 0.000  & 0.000  &  1.312 &   6    &   21   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.25   & 0      & 0.38   & 0.25   & 0.25   & 0.00   & 0.00   & 0.00   & ...    & 0.022  & 0.044  & 0.000  & 0.663  & 0.000  & 0.000  &  1.243 &  11    &  184   & 1     \\\\\n",
       "\t 0.00   & 0.69   & 0.34   & 0      & 0.34   & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & ...    & 0.000  & 0.056  & 0.000  & 0.786  & 0.000  & 0.000  &  3.728 &  61    &  261   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 0.90   & 0.00   & 0.90   & 0.00   & 0.00   & 0.90   & ...    & 0.000  & 0.000  & 0.000  & 0.000  & 0.000  & 0.000  &  2.083 &   7    &   25   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 1.42   & 0      & 0.71   & 0.35   & 0.00   & 0.35   & 0.00   & 0.71   & ...    & 0.000  & 0.102  & 0.000  & 0.357  & 0.000  & 0.000  &  1.971 &  24    &  205   & 1     \\\\\n",
       "\t 0.00   & 0.42   & 0.42   & 0      & 1.27   & 0.00   & 0.42   & 0.00   & 0.00   & 1.27   & ...    & 0.000  & 0.063  & 0.000  & 0.572  & 0.063  & 0.000  &  5.659 &  55    &  249   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 0.94   & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & ...    & 0.000  & 0.000  & 0.000  & 0.428  & 0.000  & 0.000  &  4.652 &  31    &  107   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & ...    & 0.000  & 0.000  & 0.000  & 1.975  & 0.370  & 0.000  & 35.461 &  95    &  461   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.55   & 0      & 1.11   & 0.00   & 0.18   & 0.00   & 0.00   & 0.00   & ...    & 0.000  & 0.182  & 0.000  & 0.455  & 0.000  & 0.000  &  1.320 &   4    &   70   & 1     \\\\\n",
       "\t 0.00   & 0.63   & 0.00   & 0      & 1.59   & 0.31   & 0.00   & 0.00   & 0.31   & 0.00   & ...    & 0.000  & 0.275  & 0.000  & 0.055  & 0.496  & 0.000  &  3.509 &  91    &  186   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & ...    & 0.000  & 0.729  & 0.000  & 0.729  & 0.000  & 0.000  &  3.833 &   9    &   23   & 1     \\\\\n",
       "\t 0.05   & 0.07   & 0.10   & 0      & 0.76   & 0.05   & 0.15   & 0.02   & 0.55   & 0.00   & ...    & 0.042  & 0.101  & 0.016  & 0.250  & 0.046  & 0.059  &  2.569 &  66    & 2259   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 2.94   & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & ...    & 0.404  & 0.404  & 0.000  & 0.809  & 0.000  & 0.000  &  4.857 &  12    &   34   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 1.16   & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & ...    & 0.000  & 0.133  & 0.000  & 0.667  & 0.000  & 0.000  &  1.131 &   5    &   69   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & ...    & 0.000  & 0.196  & 0.000  & 0.392  & 0.196  & 0.000  &  5.466 &  22    &   82   & 1     \\\\\n",
       "\t 0.05   & 0.07   & 0.10   & 0      & 0.76   & 0.05   & 0.15   & 0.02   & 0.55   & 0.00   & ...    & 0.042  & 0.101  & 0.016  & 0.250  & 0.046  & 0.059  &  2.565 &  66    & 2258   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & ...    & 0.000  & 0.196  & 0.000  & 0.392  & 0.196  & 0.000  &  5.466 &  22    &   82   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 0.00   & 0.00   & 1.66   & 0.00   & 0.00   & 0.00   & ...    & 0.000  & 0.000  & 0.000  & 0.368  & 0.000  & 0.000  &  2.611 &  12    &   47   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & 0.00   & ...    & 0.000  & 0.352  & 0.000  & 0.352  & 0.000  & 0.000  &  4.000 &  11    &   36   & 1     \\\\\n",
       "\t 0.00   & 0.00   & 0.00   & 0      & 0.65   & 0.00   & 0.65   & 0.00   & 0.00   & 0.00   & ...    & 0.000  & 0.459  & 0.000  & 0.091  & 0.000  & 0.000  &  2.687 &  66    &  129   & 1     \\\\\n",
       "\t ... & ... & ... & ... & ... & ... & ... & ... & ... & ... &     & ... & ... & ... & ... & ... & ... & ... & ... & ... & ...\\\\\n",
       "\t 0.00  & 0.00  & 0.46  & 0     & 0.23  & 0.23  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.082 & 0.000 & 0.082 & 0.000 & 0.000 & 1.256 &  5    &   98  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.254 & 0.000 & 0.000 & 0.000 & 0.000 & 1.000 &  1    &   13  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.18  & 0     & 0.18  & 0.18  & 0     & 0     & 0     & 0.00  & ...   & 0.033 & 0.033 & 0.000 & 0.099 & 0.000 & 0.000 & 1.489 & 11    &  137  & 0    \\\\\n",
       "\t 0.29  & 0.00  & 0.29  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.29  & ...   & 0.000 & 0.107 & 0.000 & 0.000 & 0.000 & 0.000 & 1.220 &  6    &   61  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 1.38  & ...   & 0.000 & 0.213 & 0.000 & 0.000 & 0.000 & 0.000 & 1.720 & 11    &   43  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.131 & 0.000 & 0.000 & 0.000 & 0.000 & 1.488 &  5    &   64  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 1.20  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 1.200 &  3    &   24  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.40  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.000 & 0.145 & 0.000 & 0.000 & 0.000 & 1.372 &  5    &   70  & 0    \\\\\n",
       "\t 0.27  & 0.05  & 0.10  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.607 & 0.064 & 0.036 & 0.055 & 0.000 & 0.202 & 3.766 & 43    & 1789  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 1.571 &  5    &   11  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.51  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.091 & 0.000 & 0.091 & 0.000 & 0.000 & 1.586 &  4    &   46  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 1.266 &  3    &   19  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 1.23  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.000 & 0.406 & 0.000 & 0.000 & 0.000 & 1.666 & 13    &   70  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.45  & 0     & 0.00  & 0.22  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.082 & 0.000 & 0.041 & 0.000 & 0.000 & 1.500 &  7    &  123  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.625 & 0.000 & 0.000 & 0.000 & 0.000 & 1.375 &  4    &   11  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.36  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.112 & 0.000 & 0.000 & 0.000 & 0.056 & 1.793 & 21    &  174  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.125 & 0.000 & 0.000 & 0.125 & 0.000 & 1.272 &  4    &   28  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 3.03  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 1.111 &  2    &   10  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.54  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 1.000 &  1    &   22  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.185 & 0.000 & 0.000 & 0.000 & 0.092 & 2.468 & 11    &   79  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 1.000 &  1    &    8  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 1.25  & 0     & 2.50  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.111 & 0.000 & 0.000 & 0.000 & 0.000 & 1.285 &  4    &   27  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.000 & 0.000 & 1.052 & 0.000 & 0.000 & 1.000 &  1    &    6  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.630 & 0.000 & 0.000 & 0.000 & 0.000 & 1.727 &  5    &   19  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 1.19  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 1.000 &  1    &   24  & 0    \\\\\n",
       "\t 0.31  & 0.00  & 0.62  & 0     & 0.00  & 0.31  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.232 & 0.000 & 0.000 & 0.000 & 0.000 & 1.142 &  3    &   88  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.00  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.000 & 0.000 & 0.353 & 0.000 & 0.000 & 1.555 &  4    &   14  & 0    \\\\\n",
       "\t 0.30  & 0.00  & 0.30  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.102 & 0.718 & 0.000 & 0.000 & 0.000 & 0.000 & 1.404 &  6    &  118  & 0    \\\\\n",
       "\t 0.96  & 0.00  & 0.00  & 0     & 0.32  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.057 & 0.000 & 0.000 & 0.000 & 0.000 & 1.147 &  5    &   78  & 0    \\\\\n",
       "\t 0.00  & 0.00  & 0.65  & 0     & 0.00  & 0.00  & 0     & 0     & 0     & 0.00  & ...   & 0.000 & 0.000 & 0.000 & 0.125 & 0.000 & 0.000 & 1.250 &  5    &   40  & 0    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| V1 | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | V10 | ... | V49 | V50 | V51 | V52 | V53 | V54 | V55 | V56 | V57 | V58 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0.00   | 0.64   | 0.64   | 0      | 0.32   | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | ...    | 0.000  | 0.000  | 0.000  | 0.778  | 0.000  | 0.000  |  3.756 |  61    |  278   | 1      |\n",
       "| 0.21   | 0.28   | 0.50   | 0      | 0.14   | 0.28   | 0.21   | 0.07   | 0.00   | 0.94   | ...    | 0.000  | 0.132  | 0.000  | 0.372  | 0.180  | 0.048  |  5.114 | 101    | 1028   | 1      |\n",
       "| 0.06   | 0.00   | 0.71   | 0      | 1.23   | 0.19   | 0.19   | 0.12   | 0.64   | 0.25   | ...    | 0.010  | 0.143  | 0.000  | 0.276  | 0.184  | 0.010  |  9.821 | 485    | 2259   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 0.63   | 0.00   | 0.31   | 0.63   | 0.31   | 0.63   | ...    | 0.000  | 0.137  | 0.000  | 0.137  | 0.000  | 0.000  |  3.537 |  40    |  191   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 0.63   | 0.00   | 0.31   | 0.63   | 0.31   | 0.63   | ...    | 0.000  | 0.135  | 0.000  | 0.135  | 0.000  | 0.000  |  3.537 |  40    |  191   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 1.85   | 0.00   | 0.00   | 1.85   | 0.00   | 0.00   | ...    | 0.000  | 0.223  | 0.000  | 0.000  | 0.000  | 0.000  |  3.000 |  15    |   54   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 1.92   | 0.00   | 0.00   | 0.00   | 0.00   | 0.64   | ...    | 0.000  | 0.054  | 0.000  | 0.164  | 0.054  | 0.000  |  1.671 |   4    |  112   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 1.88   | 0.00   | 0.00   | 1.88   | 0.00   | 0.00   | ...    | 0.000  | 0.206  | 0.000  | 0.000  | 0.000  | 0.000  |  2.450 |  11    |   49   | 1      |\n",
       "| 0.15   | 0.00   | 0.46   | 0      | 0.61   | 0.00   | 0.30   | 0.00   | 0.92   | 0.76   | ...    | 0.000  | 0.271  | 0.000  | 0.181  | 0.203  | 0.022  |  9.744 | 445    | 1257   | 1      |\n",
       "| 0.06   | 0.12   | 0.77   | 0      | 0.19   | 0.32   | 0.38   | 0.00   | 0.06   | 0.00   | ...    | 0.040  | 0.030  | 0.000  | 0.244  | 0.081  | 0.000  |  1.729 |  43    |  749   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 0.00   | 0.00   | 0.96   | 0.00   | 0.00   | 1.92   | ...    | 0.000  | 0.000  | 0.000  | 0.462  | 0.000  | 0.000  |  1.312 |   6    |   21   | 1      |\n",
       "| 0.00   | 0.00   | 0.25   | 0      | 0.38   | 0.25   | 0.25   | 0.00   | 0.00   | 0.00   | ...    | 0.022  | 0.044  | 0.000  | 0.663  | 0.000  | 0.000  |  1.243 |  11    |  184   | 1      |\n",
       "| 0.00   | 0.69   | 0.34   | 0      | 0.34   | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | ...    | 0.000  | 0.056  | 0.000  | 0.786  | 0.000  | 0.000  |  3.728 |  61    |  261   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 0.90   | 0.00   | 0.90   | 0.00   | 0.00   | 0.90   | ...    | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  |  2.083 |   7    |   25   | 1      |\n",
       "| 0.00   | 0.00   | 1.42   | 0      | 0.71   | 0.35   | 0.00   | 0.35   | 0.00   | 0.71   | ...    | 0.000  | 0.102  | 0.000  | 0.357  | 0.000  | 0.000  |  1.971 |  24    |  205   | 1      |\n",
       "| 0.00   | 0.42   | 0.42   | 0      | 1.27   | 0.00   | 0.42   | 0.00   | 0.00   | 1.27   | ...    | 0.000  | 0.063  | 0.000  | 0.572  | 0.063  | 0.000  |  5.659 |  55    |  249   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 0.94   | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | ...    | 0.000  | 0.000  | 0.000  | 0.428  | 0.000  | 0.000  |  4.652 |  31    |  107   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | ...    | 0.000  | 0.000  | 0.000  | 1.975  | 0.370  | 0.000  | 35.461 |  95    |  461   | 1      |\n",
       "| 0.00   | 0.00   | 0.55   | 0      | 1.11   | 0.00   | 0.18   | 0.00   | 0.00   | 0.00   | ...    | 0.000  | 0.182  | 0.000  | 0.455  | 0.000  | 0.000  |  1.320 |   4    |   70   | 1      |\n",
       "| 0.00   | 0.63   | 0.00   | 0      | 1.59   | 0.31   | 0.00   | 0.00   | 0.31   | 0.00   | ...    | 0.000  | 0.275  | 0.000  | 0.055  | 0.496  | 0.000  |  3.509 |  91    |  186   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | ...    | 0.000  | 0.729  | 0.000  | 0.729  | 0.000  | 0.000  |  3.833 |   9    |   23   | 1      |\n",
       "| 0.05   | 0.07   | 0.10   | 0      | 0.76   | 0.05   | 0.15   | 0.02   | 0.55   | 0.00   | ...    | 0.042  | 0.101  | 0.016  | 0.250  | 0.046  | 0.059  |  2.569 |  66    | 2259   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 2.94   | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | ...    | 0.404  | 0.404  | 0.000  | 0.809  | 0.000  | 0.000  |  4.857 |  12    |   34   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 1.16   | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | ...    | 0.000  | 0.133  | 0.000  | 0.667  | 0.000  | 0.000  |  1.131 |   5    |   69   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | ...    | 0.000  | 0.196  | 0.000  | 0.392  | 0.196  | 0.000  |  5.466 |  22    |   82   | 1      |\n",
       "| 0.05   | 0.07   | 0.10   | 0      | 0.76   | 0.05   | 0.15   | 0.02   | 0.55   | 0.00   | ...    | 0.042  | 0.101  | 0.016  | 0.250  | 0.046  | 0.059  |  2.565 |  66    | 2258   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | ...    | 0.000  | 0.196  | 0.000  | 0.392  | 0.196  | 0.000  |  5.466 |  22    |   82   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 0.00   | 0.00   | 1.66   | 0.00   | 0.00   | 0.00   | ...    | 0.000  | 0.000  | 0.000  | 0.368  | 0.000  | 0.000  |  2.611 |  12    |   47   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | 0.00   | ...    | 0.000  | 0.352  | 0.000  | 0.352  | 0.000  | 0.000  |  4.000 |  11    |   36   | 1      |\n",
       "| 0.00   | 0.00   | 0.00   | 0      | 0.65   | 0.00   | 0.65   | 0.00   | 0.00   | 0.00   | ...    | 0.000  | 0.459  | 0.000  | 0.091  | 0.000  | 0.000  |  2.687 |  66    |  129   | 1      |\n",
       "| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |     | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
       "| 0.00  | 0.00  | 0.46  | 0     | 0.23  | 0.23  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.082 | 0.000 | 0.082 | 0.000 | 0.000 | 1.256 |  5    |   98  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.254 | 0.000 | 0.000 | 0.000 | 0.000 | 1.000 |  1    |   13  | 0     |\n",
       "| 0.00  | 0.00  | 0.18  | 0     | 0.18  | 0.18  | 0     | 0     | 0     | 0.00  | ...   | 0.033 | 0.033 | 0.000 | 0.099 | 0.000 | 0.000 | 1.489 | 11    |  137  | 0     |\n",
       "| 0.29  | 0.00  | 0.29  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.29  | ...   | 0.000 | 0.107 | 0.000 | 0.000 | 0.000 | 0.000 | 1.220 |  6    |   61  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 1.38  | ...   | 0.000 | 0.213 | 0.000 | 0.000 | 0.000 | 0.000 | 1.720 | 11    |   43  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.131 | 0.000 | 0.000 | 0.000 | 0.000 | 1.488 |  5    |   64  | 0     |\n",
       "| 0.00  | 0.00  | 1.20  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 1.200 |  3    |   24  | 0     |\n",
       "| 0.00  | 0.00  | 0.40  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.000 | 0.145 | 0.000 | 0.000 | 0.000 | 1.372 |  5    |   70  | 0     |\n",
       "| 0.27  | 0.05  | 0.10  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.607 | 0.064 | 0.036 | 0.055 | 0.000 | 0.202 | 3.766 | 43    | 1789  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 1.571 |  5    |   11  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.51  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.091 | 0.000 | 0.091 | 0.000 | 0.000 | 1.586 |  4    |   46  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 1.266 |  3    |   19  | 0     |\n",
       "| 0.00  | 0.00  | 1.23  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.000 | 0.406 | 0.000 | 0.000 | 0.000 | 1.666 | 13    |   70  | 0     |\n",
       "| 0.00  | 0.00  | 0.45  | 0     | 0.00  | 0.22  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.082 | 0.000 | 0.041 | 0.000 | 0.000 | 1.500 |  7    |  123  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.625 | 0.000 | 0.000 | 0.000 | 0.000 | 1.375 |  4    |   11  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.36  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.112 | 0.000 | 0.000 | 0.000 | 0.056 | 1.793 | 21    |  174  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.125 | 0.000 | 0.000 | 0.125 | 0.000 | 1.272 |  4    |   28  | 0     |\n",
       "| 0.00  | 0.00  | 3.03  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 1.111 |  2    |   10  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.54  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 1.000 |  1    |   22  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.185 | 0.000 | 0.000 | 0.000 | 0.092 | 2.468 | 11    |   79  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 1.000 |  1    |    8  | 0     |\n",
       "| 0.00  | 0.00  | 1.25  | 0     | 2.50  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.111 | 0.000 | 0.000 | 0.000 | 0.000 | 1.285 |  4    |   27  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.000 | 0.000 | 1.052 | 0.000 | 0.000 | 1.000 |  1    |    6  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.630 | 0.000 | 0.000 | 0.000 | 0.000 | 1.727 |  5    |   19  | 0     |\n",
       "| 0.00  | 0.00  | 1.19  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 1.000 |  1    |   24  | 0     |\n",
       "| 0.31  | 0.00  | 0.62  | 0     | 0.00  | 0.31  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.232 | 0.000 | 0.000 | 0.000 | 0.000 | 1.142 |  3    |   88  | 0     |\n",
       "| 0.00  | 0.00  | 0.00  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.000 | 0.000 | 0.353 | 0.000 | 0.000 | 1.555 |  4    |   14  | 0     |\n",
       "| 0.30  | 0.00  | 0.30  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.102 | 0.718 | 0.000 | 0.000 | 0.000 | 0.000 | 1.404 |  6    |  118  | 0     |\n",
       "| 0.96  | 0.00  | 0.00  | 0     | 0.32  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.057 | 0.000 | 0.000 | 0.000 | 0.000 | 1.147 |  5    |   78  | 0     |\n",
       "| 0.00  | 0.00  | 0.65  | 0     | 0.00  | 0.00  | 0     | 0     | 0     | 0.00  | ...   | 0.000 | 0.000 | 0.000 | 0.125 | 0.000 | 0.000 | 1.250 |  5    |   40  | 0     |\n",
       "\n"
      ],
      "text/plain": [
       "     V1   V2   V3   V4  V5   V6   V7   V8   V9   V10  ... V49   V50   V51  \n",
       "1    0.00 0.64 0.64 0   0.32 0.00 0.00 0.00 0.00 0.00 ... 0.000 0.000 0.000\n",
       "2    0.21 0.28 0.50 0   0.14 0.28 0.21 0.07 0.00 0.94 ... 0.000 0.132 0.000\n",
       "3    0.06 0.00 0.71 0   1.23 0.19 0.19 0.12 0.64 0.25 ... 0.010 0.143 0.000\n",
       "4    0.00 0.00 0.00 0   0.63 0.00 0.31 0.63 0.31 0.63 ... 0.000 0.137 0.000\n",
       "5    0.00 0.00 0.00 0   0.63 0.00 0.31 0.63 0.31 0.63 ... 0.000 0.135 0.000\n",
       "6    0.00 0.00 0.00 0   1.85 0.00 0.00 1.85 0.00 0.00 ... 0.000 0.223 0.000\n",
       "7    0.00 0.00 0.00 0   1.92 0.00 0.00 0.00 0.00 0.64 ... 0.000 0.054 0.000\n",
       "8    0.00 0.00 0.00 0   1.88 0.00 0.00 1.88 0.00 0.00 ... 0.000 0.206 0.000\n",
       "9    0.15 0.00 0.46 0   0.61 0.00 0.30 0.00 0.92 0.76 ... 0.000 0.271 0.000\n",
       "10   0.06 0.12 0.77 0   0.19 0.32 0.38 0.00 0.06 0.00 ... 0.040 0.030 0.000\n",
       "11   0.00 0.00 0.00 0   0.00 0.00 0.96 0.00 0.00 1.92 ... 0.000 0.000 0.000\n",
       "12   0.00 0.00 0.25 0   0.38 0.25 0.25 0.00 0.00 0.00 ... 0.022 0.044 0.000\n",
       "13   0.00 0.69 0.34 0   0.34 0.00 0.00 0.00 0.00 0.00 ... 0.000 0.056 0.000\n",
       "14   0.00 0.00 0.00 0   0.90 0.00 0.90 0.00 0.00 0.90 ... 0.000 0.000 0.000\n",
       "15   0.00 0.00 1.42 0   0.71 0.35 0.00 0.35 0.00 0.71 ... 0.000 0.102 0.000\n",
       "16   0.00 0.42 0.42 0   1.27 0.00 0.42 0.00 0.00 1.27 ... 0.000 0.063 0.000\n",
       "17   0.00 0.00 0.00 0   0.94 0.00 0.00 0.00 0.00 0.00 ... 0.000 0.000 0.000\n",
       "18   0.00 0.00 0.00 0   0.00 0.00 0.00 0.00 0.00 0.00 ... 0.000 0.000 0.000\n",
       "19   0.00 0.00 0.55 0   1.11 0.00 0.18 0.00 0.00 0.00 ... 0.000 0.182 0.000\n",
       "20   0.00 0.63 0.00 0   1.59 0.31 0.00 0.00 0.31 0.00 ... 0.000 0.275 0.000\n",
       "21   0.00 0.00 0.00 0   0.00 0.00 0.00 0.00 0.00 0.00 ... 0.000 0.729 0.000\n",
       "22   0.05 0.07 0.10 0   0.76 0.05 0.15 0.02 0.55 0.00 ... 0.042 0.101 0.016\n",
       "23   0.00 0.00 0.00 0   2.94 0.00 0.00 0.00 0.00 0.00 ... 0.404 0.404 0.000\n",
       "24   0.00 0.00 0.00 0   1.16 0.00 0.00 0.00 0.00 0.00 ... 0.000 0.133 0.000\n",
       "25   0.00 0.00 0.00 0   0.00 0.00 0.00 0.00 0.00 0.00 ... 0.000 0.196 0.000\n",
       "26   0.05 0.07 0.10 0   0.76 0.05 0.15 0.02 0.55 0.00 ... 0.042 0.101 0.016\n",
       "27   0.00 0.00 0.00 0   0.00 0.00 0.00 0.00 0.00 0.00 ... 0.000 0.196 0.000\n",
       "28   0.00 0.00 0.00 0   0.00 0.00 1.66 0.00 0.00 0.00 ... 0.000 0.000 0.000\n",
       "29   0.00 0.00 0.00 0   0.00 0.00 0.00 0.00 0.00 0.00 ... 0.000 0.352 0.000\n",
       "30   0.00 0.00 0.00 0   0.65 0.00 0.65 0.00 0.00 0.00 ... 0.000 0.459 0.000\n",
       "...  ...  ...  ...  ... ...  ...  ...  ...  ...  ...      ...   ...   ...  \n",
       "4572 0.00 0.00 0.46 0   0.23 0.23 0    0    0    0.00 ... 0.000 0.082 0.000\n",
       "4573 0.00 0.00 0.00 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.254 0.000\n",
       "4574 0.00 0.00 0.18 0   0.18 0.18 0    0    0    0.00 ... 0.033 0.033 0.000\n",
       "4575 0.29 0.00 0.29 0   0.00 0.00 0    0    0    0.29 ... 0.000 0.107 0.000\n",
       "4576 0.00 0.00 0.00 0   0.00 0.00 0    0    0    1.38 ... 0.000 0.213 0.000\n",
       "4577 0.00 0.00 0.00 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.131 0.000\n",
       "4578 0.00 0.00 1.20 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.000 0.000\n",
       "4579 0.00 0.00 0.40 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.000 0.145\n",
       "4580 0.27 0.05 0.10 0   0.00 0.00 0    0    0    0.00 ... 0.607 0.064 0.036\n",
       "4581 0.00 0.00 0.00 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.000 0.000\n",
       "4582 0.00 0.00 0.00 0   0.00 0.51 0    0    0    0.00 ... 0.000 0.091 0.000\n",
       "4583 0.00 0.00 0.00 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.000 0.000\n",
       "4584 0.00 0.00 1.23 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.000 0.406\n",
       "4585 0.00 0.00 0.45 0   0.00 0.22 0    0    0    0.00 ... 0.000 0.082 0.000\n",
       "4586 0.00 0.00 0.00 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.625 0.000\n",
       "4587 0.00 0.00 0.00 0   0.36 0.00 0    0    0    0.00 ... 0.000 0.112 0.000\n",
       "4588 0.00 0.00 0.00 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.125 0.000\n",
       "4589 0.00 0.00 3.03 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.000 0.000\n",
       "4590 0.00 0.00 0.00 0   0.54 0.00 0    0    0    0.00 ... 0.000 0.000 0.000\n",
       "4591 0.00 0.00 0.00 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.185 0.000\n",
       "4592 0.00 0.00 0.00 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.000 0.000\n",
       "4593 0.00 0.00 1.25 0   2.50 0.00 0    0    0    0.00 ... 0.000 0.111 0.000\n",
       "4594 0.00 0.00 0.00 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.000 0.000\n",
       "4595 0.00 0.00 0.00 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.630 0.000\n",
       "4596 0.00 0.00 1.19 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.000 0.000\n",
       "4597 0.31 0.00 0.62 0   0.00 0.31 0    0    0    0.00 ... 0.000 0.232 0.000\n",
       "4598 0.00 0.00 0.00 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.000 0.000\n",
       "4599 0.30 0.00 0.30 0   0.00 0.00 0    0    0    0.00 ... 0.102 0.718 0.000\n",
       "4600 0.96 0.00 0.00 0   0.32 0.00 0    0    0    0.00 ... 0.000 0.057 0.000\n",
       "4601 0.00 0.00 0.65 0   0.00 0.00 0    0    0    0.00 ... 0.000 0.000 0.000\n",
       "     V52   V53   V54   V55    V56 V57  V58\n",
       "1    0.778 0.000 0.000  3.756  61  278 1  \n",
       "2    0.372 0.180 0.048  5.114 101 1028 1  \n",
       "3    0.276 0.184 0.010  9.821 485 2259 1  \n",
       "4    0.137 0.000 0.000  3.537  40  191 1  \n",
       "5    0.135 0.000 0.000  3.537  40  191 1  \n",
       "6    0.000 0.000 0.000  3.000  15   54 1  \n",
       "7    0.164 0.054 0.000  1.671   4  112 1  \n",
       "8    0.000 0.000 0.000  2.450  11   49 1  \n",
       "9    0.181 0.203 0.022  9.744 445 1257 1  \n",
       "10   0.244 0.081 0.000  1.729  43  749 1  \n",
       "11   0.462 0.000 0.000  1.312   6   21 1  \n",
       "12   0.663 0.000 0.000  1.243  11  184 1  \n",
       "13   0.786 0.000 0.000  3.728  61  261 1  \n",
       "14   0.000 0.000 0.000  2.083   7   25 1  \n",
       "15   0.357 0.000 0.000  1.971  24  205 1  \n",
       "16   0.572 0.063 0.000  5.659  55  249 1  \n",
       "17   0.428 0.000 0.000  4.652  31  107 1  \n",
       "18   1.975 0.370 0.000 35.461  95  461 1  \n",
       "19   0.455 0.000 0.000  1.320   4   70 1  \n",
       "20   0.055 0.496 0.000  3.509  91  186 1  \n",
       "21   0.729 0.000 0.000  3.833   9   23 1  \n",
       "22   0.250 0.046 0.059  2.569  66 2259 1  \n",
       "23   0.809 0.000 0.000  4.857  12   34 1  \n",
       "24   0.667 0.000 0.000  1.131   5   69 1  \n",
       "25   0.392 0.196 0.000  5.466  22   82 1  \n",
       "26   0.250 0.046 0.059  2.565  66 2258 1  \n",
       "27   0.392 0.196 0.000  5.466  22   82 1  \n",
       "28   0.368 0.000 0.000  2.611  12   47 1  \n",
       "29   0.352 0.000 0.000  4.000  11   36 1  \n",
       "30   0.091 0.000 0.000  2.687  66  129 1  \n",
       "...  ...   ...   ...   ...    ... ...  ...\n",
       "4572 0.082 0.000 0.000 1.256   5    98 0  \n",
       "4573 0.000 0.000 0.000 1.000   1    13 0  \n",
       "4574 0.099 0.000 0.000 1.489  11   137 0  \n",
       "4575 0.000 0.000 0.000 1.220   6    61 0  \n",
       "4576 0.000 0.000 0.000 1.720  11    43 0  \n",
       "4577 0.000 0.000 0.000 1.488   5    64 0  \n",
       "4578 0.000 0.000 0.000 1.200   3    24 0  \n",
       "4579 0.000 0.000 0.000 1.372   5    70 0  \n",
       "4580 0.055 0.000 0.202 3.766  43  1789 0  \n",
       "4581 0.000 0.000 0.000 1.571   5    11 0  \n",
       "4582 0.091 0.000 0.000 1.586   4    46 0  \n",
       "4583 0.000 0.000 0.000 1.266   3    19 0  \n",
       "4584 0.000 0.000 0.000 1.666  13    70 0  \n",
       "4585 0.041 0.000 0.000 1.500   7   123 0  \n",
       "4586 0.000 0.000 0.000 1.375   4    11 0  \n",
       "4587 0.000 0.000 0.056 1.793  21   174 0  \n",
       "4588 0.000 0.125 0.000 1.272   4    28 0  \n",
       "4589 0.000 0.000 0.000 1.111   2    10 0  \n",
       "4590 0.000 0.000 0.000 1.000   1    22 0  \n",
       "4591 0.000 0.000 0.092 2.468  11    79 0  \n",
       "4592 0.000 0.000 0.000 1.000   1     8 0  \n",
       "4593 0.000 0.000 0.000 1.285   4    27 0  \n",
       "4594 1.052 0.000 0.000 1.000   1     6 0  \n",
       "4595 0.000 0.000 0.000 1.727   5    19 0  \n",
       "4596 0.000 0.000 0.000 1.000   1    24 0  \n",
       "4597 0.000 0.000 0.000 1.142   3    88 0  \n",
       "4598 0.353 0.000 0.000 1.555   4    14 0  \n",
       "4599 0.000 0.000 0.000 1.404   6   118 0  \n",
       "4600 0.000 0.000 0.000 1.147   5    78 0  \n",
       "4601 0.125 0.000 0.000 1.250   5    40 0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data1 = read.delim(\"spambase.data\", header = FALSE, sep = \",\", dec = \".\")\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t4601 obs. of  58 variables:\n",
      " $ V1 : num  0 0.21 0.06 0 0 0 0 0 0.15 0.06 ...\n",
      " $ V2 : num  0.64 0.28 0 0 0 0 0 0 0 0.12 ...\n",
      " $ V3 : num  0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ...\n",
      " $ V4 : num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V5 : num  0.32 0.14 1.23 0.63 0.63 1.85 1.92 1.88 0.61 0.19 ...\n",
      " $ V6 : num  0 0.28 0.19 0 0 0 0 0 0 0.32 ...\n",
      " $ V7 : num  0 0.21 0.19 0.31 0.31 0 0 0 0.3 0.38 ...\n",
      " $ V8 : num  0 0.07 0.12 0.63 0.63 1.85 0 1.88 0 0 ...\n",
      " $ V9 : num  0 0 0.64 0.31 0.31 0 0 0 0.92 0.06 ...\n",
      " $ V10: num  0 0.94 0.25 0.63 0.63 0 0.64 0 0.76 0 ...\n",
      " $ V11: num  0 0.21 0.38 0.31 0.31 0 0.96 0 0.76 0 ...\n",
      " $ V12: num  0.64 0.79 0.45 0.31 0.31 0 1.28 0 0.92 0.64 ...\n",
      " $ V13: num  0 0.65 0.12 0.31 0.31 0 0 0 0 0.25 ...\n",
      " $ V14: num  0 0.21 0 0 0 0 0 0 0 0 ...\n",
      " $ V15: num  0 0.14 1.75 0 0 0 0 0 0 0.12 ...\n",
      " $ V16: num  0.32 0.14 0.06 0.31 0.31 0 0.96 0 0 0 ...\n",
      " $ V17: num  0 0.07 0.06 0 0 0 0 0 0 0 ...\n",
      " $ V18: num  1.29 0.28 1.03 0 0 0 0.32 0 0.15 0.12 ...\n",
      " $ V19: num  1.93 3.47 1.36 3.18 3.18 0 3.85 0 1.23 1.67 ...\n",
      " $ V20: num  0 0 0.32 0 0 0 0 0 3.53 0.06 ...\n",
      " $ V21: num  0.96 1.59 0.51 0.31 0.31 0 0.64 0 2 0.71 ...\n",
      " $ V22: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V23: num  0 0.43 1.16 0 0 0 0 0 0 0.19 ...\n",
      " $ V24: num  0 0.43 0.06 0 0 0 0 0 0.15 0 ...\n",
      " $ V25: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V26: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V27: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V28: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V29: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V30: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V31: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V32: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V33: num  0 0 0 0 0 0 0 0 0.15 0 ...\n",
      " $ V34: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V35: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V36: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V37: num  0 0.07 0 0 0 0 0 0 0 0 ...\n",
      " $ V38: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V39: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V40: num  0 0 0.06 0 0 0 0 0 0 0 ...\n",
      " $ V41: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V42: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V43: num  0 0 0.12 0 0 0 0 0 0.3 0 ...\n",
      " $ V44: num  0 0 0 0 0 0 0 0 0 0.06 ...\n",
      " $ V45: num  0 0 0.06 0 0 0 0 0 0 0 ...\n",
      " $ V46: num  0 0 0.06 0 0 0 0 0 0 0 ...\n",
      " $ V47: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V48: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V49: num  0 0 0.01 0 0 0 0 0 0 0.04 ...\n",
      " $ V50: num  0 0.132 0.143 0.137 0.135 0.223 0.054 0.206 0.271 0.03 ...\n",
      " $ V51: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V52: num  0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ...\n",
      " $ V53: num  0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ...\n",
      " $ V54: num  0 0.048 0.01 0 0 0 0 0 0.022 0 ...\n",
      " $ V55: num  3.76 5.11 9.82 3.54 3.54 ...\n",
      " $ V56: int  61 101 485 40 40 15 4 11 445 43 ...\n",
      " $ V57: int  278 1028 2259 191 191 54 112 49 1257 749 ...\n",
      " $ V58: int  1 1 1 1 1 1 1 1 1 1 ...\n"
     ]
    }
   ],
   "source": [
    "str(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "trainIndex = createDataPartition(data1$V58, p = 0.7, list = FALSE)\n",
    "data1_train_1 = data1[trainIndex, ]\n",
    "data1_test_1 = data1[-trainIndex, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"You are trying to do regression and your outcome only has two possible values Are you trying to do classification? If so, use a 2 level factor as your outcome column.\"Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "\"There were missing values in resampled performance measures.\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "3221 samples\n",
       "  57 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 2899, 2899, 2899, 2899, 2899, 2898, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  lambda  RMSE       Rsquared   MAE      \n",
       "  0.0     0.3439133  0.5185148  0.2693482\n",
       "  0.5     0.4902058        NaN  0.4804842\n",
       "  1.0     0.4902058        NaN  0.4804842\n",
       "\n",
       "Tuning parameter 'alpha' was held constant at a value of 1\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were alpha = 1 and lambda = 0."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(2)\n",
    "data1_glm_model <- train(V58 ~ ., data = data1_train_1, method = \"glmnet\", trControl = fitControl, tuneGrid = glm_grid)\n",
    "data1_glm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"You are trying to do regression and your outcome only has two possible values Are you trying to do classification? If so, use a 2 level factor as your outcome column.\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "CART \n",
       "\n",
       "3221 samples\n",
       "  57 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 2899, 2898, 2899, 2899, 2899, 2899, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  cp    RMSE       Rsquared   MAE      \n",
       "  0.01  0.2841334  0.6629347  0.1561977\n",
       "  0.05  0.3331575  0.5374268  0.2183113\n",
       "  0.10  0.3619262  0.4531305  0.2608803\n",
       "\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final value used for the model was cp = 0.01."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data1_dt_model <- train(V58 ~ ., data = data1_train_1, method = \"rpart\", trControl = fitControl, tuneGrid = dt_grid)\n",
    "data1_dt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"You are trying to do regression and your outcome only has two possible values Are you trying to do classification? If so, use a 2 level factor as your outcome column.\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"Warning message in randomForest.default(x, y, mtry = param$mtry, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "3221 samples\n",
       "  57 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 2899, 2899, 2899, 2899, 2899, 2899, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  RMSE       Rsquared   MAE      \n",
       "  3     0.2173973  0.8159772  0.1386277\n",
       "  5     0.2108893  0.8228017  0.1255583\n",
       "  7     0.2087809  0.8247960  0.1201925\n",
       "\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final value used for the model was mtry = 7."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data1_rf_model = train(V58 ~ ., data = data1_train_1, method = \"rf\", trControl = fitControl,tuneGrid = rf_grid)\n",
    "data1_rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"You are trying to do regression and your outcome only has two possible values Are you trying to do classification? If so, use a 2 level factor as your outcome column.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2245             nan     0.0500    0.0154\n",
      "     2        0.2096             nan     0.0500    0.0143\n",
      "     3        0.1967             nan     0.0500    0.0125\n",
      "     4        0.1847             nan     0.0500    0.0115\n",
      "     5        0.1733             nan     0.0500    0.0109\n",
      "     6        0.1635             nan     0.0500    0.0092\n",
      "     7        0.1544             nan     0.0500    0.0088\n",
      "     8        0.1462             nan     0.0500    0.0077\n",
      "     9        0.1382             nan     0.0500    0.0077\n",
      "    10        0.1310             nan     0.0500    0.0067\n",
      "    20        0.0859             nan     0.0500    0.0028\n",
      "    40        0.0526             nan     0.0500    0.0006\n",
      "    60        0.0427             nan     0.0500    0.0002\n",
      "    80        0.0388             nan     0.0500   -0.0000\n",
      "   100        0.0364             nan     0.0500    0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2240             nan     0.0500    0.0155\n",
      "     2        0.2085             nan     0.0500    0.0142\n",
      "     3        0.1943             nan     0.0500    0.0136\n",
      "     4        0.1813             nan     0.0500    0.0128\n",
      "     5        0.1694             nan     0.0500    0.0111\n",
      "     6        0.1585             nan     0.0500    0.0104\n",
      "     7        0.1489             nan     0.0500    0.0091\n",
      "     8        0.1401             nan     0.0500    0.0080\n",
      "     9        0.1319             nan     0.0500    0.0077\n",
      "    10        0.1242             nan     0.0500    0.0073\n",
      "    20        0.0760             nan     0.0500    0.0027\n",
      "    40        0.0431             nan     0.0500    0.0005\n",
      "    60        0.0331             nan     0.0500    0.0001\n",
      "    80        0.0290             nan     0.0500    0.0000\n",
      "   100        0.0262             nan     0.0500   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2084             nan     0.1000    0.0314\n",
      "     2        0.1828             nan     0.1000    0.0240\n",
      "     3        0.1618             nan     0.1000    0.0203\n",
      "     4        0.1441             nan     0.1000    0.0165\n",
      "     5        0.1302             nan     0.1000    0.0131\n",
      "     6        0.1178             nan     0.1000    0.0114\n",
      "     7        0.1079             nan     0.1000    0.0089\n",
      "     8        0.0993             nan     0.1000    0.0078\n",
      "     9        0.0916             nan     0.1000    0.0073\n",
      "    10        0.0846             nan     0.1000    0.0066\n",
      "    20        0.0535             nan     0.1000    0.0009\n",
      "    40        0.0402             nan     0.1000    0.0000\n",
      "    60        0.0358             nan     0.1000   -0.0001\n",
      "    80        0.0325             nan     0.1000   -0.0001\n",
      "   100        0.0300             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2069             nan     0.1000    0.0319\n",
      "     2        0.1794             nan     0.1000    0.0266\n",
      "     3        0.1566             nan     0.1000    0.0214\n",
      "     4        0.1379             nan     0.1000    0.0181\n",
      "     5        0.1229             nan     0.1000    0.0137\n",
      "     6        0.1093             nan     0.1000    0.0130\n",
      "     7        0.0982             nan     0.1000    0.0102\n",
      "     8        0.0896             nan     0.1000    0.0081\n",
      "     9        0.0817             nan     0.1000    0.0067\n",
      "    10        0.0754             nan     0.1000    0.0055\n",
      "    20        0.0438             nan     0.1000    0.0008\n",
      "    40        0.0298             nan     0.1000   -0.0001\n",
      "    60        0.0246             nan     0.1000    0.0000\n",
      "    80        0.0213             nan     0.1000   -0.0001\n",
      "   100        0.0185             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2245             nan     0.0500    0.0150\n",
      "     2        0.2097             nan     0.0500    0.0144\n",
      "     3        0.1971             nan     0.0500    0.0120\n",
      "     4        0.1852             nan     0.0500    0.0111\n",
      "     5        0.1739             nan     0.0500    0.0109\n",
      "     6        0.1636             nan     0.0500    0.0098\n",
      "     7        0.1549             nan     0.0500    0.0081\n",
      "     8        0.1464             nan     0.0500    0.0082\n",
      "     9        0.1393             nan     0.0500    0.0065\n",
      "    10        0.1327             nan     0.0500    0.0062\n",
      "    20        0.0860             nan     0.0500    0.0027\n",
      "    40        0.0537             nan     0.0500    0.0005\n",
      "    60        0.0434             nan     0.0500    0.0002\n",
      "    80        0.0389             nan     0.0500    0.0001\n",
      "   100        0.0365             nan     0.0500   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2229             nan     0.0500    0.0162\n",
      "     2        0.2073             nan     0.0500    0.0153\n",
      "     3        0.1933             nan     0.0500    0.0130\n",
      "     4        0.1804             nan     0.0500    0.0122\n",
      "     5        0.1684             nan     0.0500    0.0107\n",
      "     6        0.1577             nan     0.0500    0.0103\n",
      "     7        0.1484             nan     0.0500    0.0090\n",
      "     8        0.1389             nan     0.0500    0.0091\n",
      "     9        0.1308             nan     0.0500    0.0076\n",
      "    10        0.1240             nan     0.0500    0.0064\n",
      "    20        0.0762             nan     0.0500    0.0027\n",
      "    40        0.0431             nan     0.0500    0.0005\n",
      "    60        0.0329             nan     0.0500    0.0002\n",
      "    80        0.0281             nan     0.0500    0.0000\n",
      "   100        0.0252             nan     0.0500   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2082             nan     0.1000    0.0306\n",
      "     2        0.1834             nan     0.1000    0.0238\n",
      "     3        0.1619             nan     0.1000    0.0211\n",
      "     4        0.1444             nan     0.1000    0.0165\n",
      "     5        0.1293             nan     0.1000    0.0142\n",
      "     6        0.1173             nan     0.1000    0.0110\n",
      "     7        0.1066             nan     0.1000    0.0102\n",
      "     8        0.0976             nan     0.1000    0.0079\n",
      "     9        0.0905             nan     0.1000    0.0064\n",
      "    10        0.0845             nan     0.1000    0.0053\n",
      "    20        0.0523             nan     0.1000    0.0012\n",
      "    40        0.0389             nan     0.1000   -0.0001\n",
      "    60        0.0343             nan     0.1000   -0.0000\n",
      "    80        0.0310             nan     0.1000   -0.0001\n",
      "   100        0.0287             nan     0.1000   -0.0002\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2075             nan     0.1000    0.0313\n",
      "     2        0.1800             nan     0.1000    0.0264\n",
      "     3        0.1583             nan     0.1000    0.0202\n",
      "     4        0.1397             nan     0.1000    0.0177\n",
      "     5        0.1239             nan     0.1000    0.0145\n",
      "     6        0.1107             nan     0.1000    0.0121\n",
      "     7        0.0994             nan     0.1000    0.0098\n",
      "     8        0.0902             nan     0.1000    0.0084\n",
      "     9        0.0827             nan     0.1000    0.0067\n",
      "    10        0.0761             nan     0.1000    0.0058\n",
      "    20        0.0442             nan     0.1000    0.0009\n",
      "    40        0.0287             nan     0.1000   -0.0001\n",
      "    60        0.0238             nan     0.1000    0.0000\n",
      "    80        0.0202             nan     0.1000   -0.0001\n",
      "   100        0.0177             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2240             nan     0.0500    0.0154\n",
      "     2        0.2100             nan     0.0500    0.0136\n",
      "     3        0.1972             nan     0.0500    0.0121\n",
      "     4        0.1849             nan     0.0500    0.0122\n",
      "     5        0.1734             nan     0.0500    0.0113\n",
      "     6        0.1631             nan     0.0500    0.0095\n",
      "     7        0.1537             nan     0.0500    0.0090\n",
      "     8        0.1457             nan     0.0500    0.0075\n",
      "     9        0.1386             nan     0.0500    0.0069\n",
      "    10        0.1315             nan     0.0500    0.0068\n",
      "    20        0.0854             nan     0.0500    0.0030\n",
      "    40        0.0531             nan     0.0500    0.0009\n",
      "    60        0.0430             nan     0.0500    0.0001\n",
      "    80        0.0387             nan     0.0500    0.0000\n",
      "   100        0.0362             nan     0.0500   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2227             nan     0.0500    0.0164\n",
      "     2        0.2080             nan     0.0500    0.0136\n",
      "     3        0.1938             nan     0.0500    0.0133\n",
      "     4        0.1809             nan     0.0500    0.0123\n",
      "     5        0.1693             nan     0.0500    0.0109\n",
      "     6        0.1587             nan     0.0500    0.0100\n",
      "     7        0.1490             nan     0.0500    0.0090\n",
      "     8        0.1398             nan     0.0500    0.0086\n",
      "     9        0.1316             nan     0.0500    0.0075\n",
      "    10        0.1243             nan     0.0500    0.0066\n",
      "    20        0.0764             nan     0.0500    0.0029\n",
      "    40        0.0432             nan     0.0500    0.0004\n",
      "    60        0.0327             nan     0.0500    0.0001\n",
      "    80        0.0281             nan     0.0500    0.0000\n",
      "   100        0.0251             nan     0.0500   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2089             nan     0.1000    0.0294\n",
      "     2        0.1829             nan     0.1000    0.0245\n",
      "     3        0.1615             nan     0.1000    0.0204\n",
      "     4        0.1448             nan     0.1000    0.0162\n",
      "     5        0.1300             nan     0.1000    0.0138\n",
      "     6        0.1172             nan     0.1000    0.0113\n",
      "     7        0.1072             nan     0.1000    0.0096\n",
      "     8        0.0984             nan     0.1000    0.0083\n",
      "     9        0.0916             nan     0.1000    0.0062\n",
      "    10        0.0859             nan     0.1000    0.0048\n",
      "    20        0.0533             nan     0.1000    0.0014\n",
      "    40        0.0394             nan     0.1000    0.0000\n",
      "    60        0.0350             nan     0.1000   -0.0001\n",
      "    80        0.0315             nan     0.1000    0.0000\n",
      "   100        0.0287             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2060             nan     0.1000    0.0330\n",
      "     2        0.1784             nan     0.1000    0.0264\n",
      "     3        0.1556             nan     0.1000    0.0215\n",
      "     4        0.1368             nan     0.1000    0.0171\n",
      "     5        0.1216             nan     0.1000    0.0141\n",
      "     6        0.1087             nan     0.1000    0.0122\n",
      "     7        0.0974             nan     0.1000    0.0108\n",
      "     8        0.0883             nan     0.1000    0.0083\n",
      "     9        0.0811             nan     0.1000    0.0064\n",
      "    10        0.0748             nan     0.1000    0.0053\n",
      "    20        0.0432             nan     0.1000    0.0011\n",
      "    40        0.0286             nan     0.1000    0.0000\n",
      "    60        0.0239             nan     0.1000   -0.0001\n",
      "    80        0.0204             nan     0.1000   -0.0001\n",
      "   100        0.0177             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2232             nan     0.0500    0.0157\n",
      "     2        0.2084             nan     0.0500    0.0143\n",
      "     3        0.1944             nan     0.0500    0.0133\n",
      "     4        0.1820             nan     0.0500    0.0116\n",
      "     5        0.1714             nan     0.0500    0.0101\n",
      "     6        0.1612             nan     0.0500    0.0096\n",
      "     7        0.1525             nan     0.0500    0.0081\n",
      "     8        0.1443             nan     0.0500    0.0078\n",
      "     9        0.1364             nan     0.0500    0.0076\n",
      "    10        0.1292             nan     0.0500    0.0068\n",
      "    20        0.0837             nan     0.0500    0.0027\n",
      "    40        0.0507             nan     0.0500    0.0007\n",
      "    60        0.0407             nan     0.0500    0.0002\n",
      "    80        0.0367             nan     0.0500   -0.0000\n",
      "   100        0.0344             nan     0.0500    0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2221             nan     0.0500    0.0165\n",
      "     2        0.2066             nan     0.0500    0.0149\n",
      "     3        0.1925             nan     0.0500    0.0134\n",
      "     4        0.1795             nan     0.0500    0.0120\n",
      "     5        0.1677             nan     0.0500    0.0114\n",
      "     6        0.1569             nan     0.0500    0.0103\n",
      "     7        0.1468             nan     0.0500    0.0093\n",
      "     8        0.1376             nan     0.0500    0.0086\n",
      "     9        0.1296             nan     0.0500    0.0077\n",
      "    10        0.1218             nan     0.0500    0.0072\n",
      "    20        0.0747             nan     0.0500    0.0026\n",
      "    40        0.0413             nan     0.0500    0.0007\n",
      "    60        0.0311             nan     0.0500    0.0002\n",
      "    80        0.0268             nan     0.0500   -0.0000\n",
      "   100        0.0242             nan     0.0500   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2088             nan     0.1000    0.0291\n",
      "     2        0.1822             nan     0.1000    0.0261\n",
      "     3        0.1608             nan     0.1000    0.0207\n",
      "     4        0.1433             nan     0.1000    0.0166\n",
      "     5        0.1287             nan     0.1000    0.0136\n",
      "     6        0.1160             nan     0.1000    0.0118\n",
      "     7        0.1053             nan     0.1000    0.0099\n",
      "     8        0.0964             nan     0.1000    0.0082\n",
      "     9        0.0883             nan     0.1000    0.0076\n",
      "    10        0.0821             nan     0.1000    0.0054\n",
      "    20        0.0503             nan     0.1000    0.0015\n",
      "    40        0.0367             nan     0.1000    0.0001\n",
      "    60        0.0325             nan     0.1000   -0.0001\n",
      "    80        0.0294             nan     0.1000   -0.0001\n",
      "   100        0.0268             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2056             nan     0.1000    0.0326\n",
      "     2        0.1788             nan     0.1000    0.0253\n",
      "     3        0.1564             nan     0.1000    0.0212\n",
      "     4        0.1364             nan     0.1000    0.0188\n",
      "     5        0.1206             nan     0.1000    0.0149\n",
      "     6        0.1074             nan     0.1000    0.0120\n",
      "     7        0.0968             nan     0.1000    0.0093\n",
      "     8        0.0874             nan     0.1000    0.0085\n",
      "     9        0.0797             nan     0.1000    0.0067\n",
      "    10        0.0729             nan     0.1000    0.0060\n",
      "    20        0.0417             nan     0.1000    0.0008\n",
      "    40        0.0273             nan     0.1000   -0.0001\n",
      "    60        0.0228             nan     0.1000   -0.0001\n",
      "    80        0.0191             nan     0.1000   -0.0001\n",
      "   100        0.0166             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2240             nan     0.0500    0.0158\n",
      "     2        0.2099             nan     0.0500    0.0137\n",
      "     3        0.1967             nan     0.0500    0.0125\n",
      "     4        0.1848             nan     0.0500    0.0118\n",
      "     5        0.1740             nan     0.0500    0.0104\n",
      "     6        0.1641             nan     0.0500    0.0095\n",
      "     7        0.1550             nan     0.0500    0.0087\n",
      "     8        0.1468             nan     0.0500    0.0082\n",
      "     9        0.1392             nan     0.0500    0.0073\n",
      "    10        0.1323             nan     0.0500    0.0066\n",
      "    20        0.0859             nan     0.0500    0.0027\n",
      "    40        0.0538             nan     0.0500    0.0007\n",
      "    60        0.0438             nan     0.0500    0.0002\n",
      "    80        0.0391             nan     0.0500    0.0000\n",
      "   100        0.0365             nan     0.0500   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2229             nan     0.0500    0.0166\n",
      "     2        0.2073             nan     0.0500    0.0150\n",
      "     3        0.1932             nan     0.0500    0.0135\n",
      "     4        0.1807             nan     0.0500    0.0121\n",
      "     5        0.1690             nan     0.0500    0.0112\n",
      "     6        0.1584             nan     0.0500    0.0097\n",
      "     7        0.1487             nan     0.0500    0.0091\n",
      "     8        0.1398             nan     0.0500    0.0085\n",
      "     9        0.1316             nan     0.0500    0.0074\n",
      "    10        0.1243             nan     0.0500    0.0068\n",
      "    20        0.0759             nan     0.0500    0.0031\n",
      "    40        0.0438             nan     0.0500    0.0004\n",
      "    60        0.0334             nan     0.0500    0.0002\n",
      "    80        0.0284             nan     0.0500   -0.0000\n",
      "   100        0.0257             nan     0.0500   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2103             nan     0.1000    0.0297\n",
      "     2        0.1837             nan     0.1000    0.0259\n",
      "     3        0.1621             nan     0.1000    0.0208\n",
      "     4        0.1451             nan     0.1000    0.0165\n",
      "     5        0.1309             nan     0.1000    0.0132\n",
      "     6        0.1187             nan     0.1000    0.0113\n",
      "     7        0.1085             nan     0.1000    0.0094\n",
      "     8        0.0995             nan     0.1000    0.0085\n",
      "     9        0.0919             nan     0.1000    0.0074\n",
      "    10        0.0855             nan     0.1000    0.0060\n",
      "    20        0.0526             nan     0.1000    0.0016\n",
      "    40        0.0392             nan     0.1000    0.0001\n",
      "    60        0.0347             nan     0.1000   -0.0000\n",
      "    80        0.0316             nan     0.1000   -0.0001\n",
      "   100        0.0289             nan     0.1000    0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2076             nan     0.1000    0.0308\n",
      "     2        0.1807             nan     0.1000    0.0252\n",
      "     3        0.1584             nan     0.1000    0.0209\n",
      "     4        0.1395             nan     0.1000    0.0175\n",
      "     5        0.1232             nan     0.1000    0.0150\n",
      "     6        0.1112             nan     0.1000    0.0104\n",
      "     7        0.1003             nan     0.1000    0.0102\n",
      "     8        0.0914             nan     0.1000    0.0079\n",
      "     9        0.0838             nan     0.1000    0.0068\n",
      "    10        0.0767             nan     0.1000    0.0063\n",
      "    20        0.0430             nan     0.1000    0.0010\n",
      "    40        0.0292             nan     0.1000    0.0001\n",
      "    60        0.0241             nan     0.1000    0.0001\n",
      "    80        0.0206             nan     0.1000   -0.0000\n",
      "   100        0.0179             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2254             nan     0.0500    0.0151\n",
      "     2        0.2108             nan     0.0500    0.0140\n",
      "     3        0.1977             nan     0.0500    0.0127\n",
      "     4        0.1859             nan     0.0500    0.0114\n",
      "     5        0.1753             nan     0.0500    0.0101\n",
      "     6        0.1654             nan     0.0500    0.0095\n",
      "     7        0.1560             nan     0.0500    0.0091\n",
      "     8        0.1476             nan     0.0500    0.0078\n",
      "     9        0.1398             nan     0.0500    0.0075\n",
      "    10        0.1331             nan     0.0500    0.0065\n",
      "    20        0.0866             nan     0.0500    0.0024\n",
      "    40        0.0536             nan     0.0500    0.0007\n",
      "    60        0.0438             nan     0.0500    0.0000\n",
      "    80        0.0398             nan     0.0500    0.0000\n",
      "   100        0.0370             nan     0.0500   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2240             nan     0.0500    0.0157\n",
      "     2        0.2086             nan     0.0500    0.0150\n",
      "     3        0.1943             nan     0.0500    0.0136\n",
      "     4        0.1812             nan     0.0500    0.0126\n",
      "     5        0.1696             nan     0.0500    0.0114\n",
      "     6        0.1586             nan     0.0500    0.0105\n",
      "     7        0.1489             nan     0.0500    0.0093\n",
      "     8        0.1402             nan     0.0500    0.0082\n",
      "     9        0.1320             nan     0.0500    0.0076\n",
      "    10        0.1245             nan     0.0500    0.0067\n",
      "    20        0.0765             nan     0.0500    0.0028\n",
      "    40        0.0443             nan     0.0500    0.0005\n",
      "    60        0.0343             nan     0.0500    0.0002\n",
      "    80        0.0297             nan     0.0500   -0.0000\n",
      "   100        0.0266             nan     0.0500    0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2092             nan     0.1000    0.0303\n",
      "     2        0.1836             nan     0.1000    0.0246\n",
      "     3        0.1633             nan     0.1000    0.0194\n",
      "     4        0.1458             nan     0.1000    0.0164\n",
      "     5        0.1298             nan     0.1000    0.0151\n",
      "     6        0.1174             nan     0.1000    0.0114\n",
      "     7        0.1071             nan     0.1000    0.0098\n",
      "     8        0.0987             nan     0.1000    0.0078\n",
      "     9        0.0916             nan     0.1000    0.0064\n",
      "    10        0.0853             nan     0.1000    0.0055\n",
      "    20        0.0528             nan     0.1000    0.0014\n",
      "    40        0.0400             nan     0.1000    0.0001\n",
      "    60        0.0350             nan     0.1000    0.0000\n",
      "    80        0.0315             nan     0.1000   -0.0002\n",
      "   100        0.0290             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2075             nan     0.1000    0.0316\n",
      "     2        0.1798             nan     0.1000    0.0265\n",
      "     3        0.1574             nan     0.1000    0.0209\n",
      "     4        0.1386             nan     0.1000    0.0179\n",
      "     5        0.1232             nan     0.1000    0.0146\n",
      "     6        0.1091             nan     0.1000    0.0131\n",
      "     7        0.0977             nan     0.1000    0.0102\n",
      "     8        0.0884             nan     0.1000    0.0080\n",
      "     9        0.0806             nan     0.1000    0.0070\n",
      "    10        0.0745             nan     0.1000    0.0052\n",
      "    20        0.0441             nan     0.1000    0.0009\n",
      "    40        0.0296             nan     0.1000    0.0001\n",
      "    60        0.0241             nan     0.1000   -0.0001\n",
      "    80        0.0208             nan     0.1000   -0.0001\n",
      "   100        0.0180             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2243             nan     0.0500    0.0155\n",
      "     2        0.2095             nan     0.0500    0.0147\n",
      "     3        0.1960             nan     0.0500    0.0129\n",
      "     4        0.1837             nan     0.0500    0.0120\n",
      "     5        0.1723             nan     0.0500    0.0107\n",
      "     6        0.1627             nan     0.0500    0.0095\n",
      "     7        0.1536             nan     0.0500    0.0088\n",
      "     8        0.1449             nan     0.0500    0.0084\n",
      "     9        0.1372             nan     0.0500    0.0073\n",
      "    10        0.1303             nan     0.0500    0.0063\n",
      "    20        0.0841             nan     0.0500    0.0028\n",
      "    40        0.0519             nan     0.0500    0.0006\n",
      "    60        0.0418             nan     0.0500    0.0002\n",
      "    80        0.0377             nan     0.0500    0.0000\n",
      "   100        0.0353             nan     0.0500   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2237             nan     0.0500    0.0158\n",
      "     2        0.2078             nan     0.0500    0.0152\n",
      "     3        0.1935             nan     0.0500    0.0135\n",
      "     4        0.1806             nan     0.0500    0.0119\n",
      "     5        0.1688             nan     0.0500    0.0112\n",
      "     6        0.1580             nan     0.0500    0.0101\n",
      "     7        0.1480             nan     0.0500    0.0096\n",
      "     8        0.1390             nan     0.0500    0.0083\n",
      "     9        0.1309             nan     0.0500    0.0077\n",
      "    10        0.1233             nan     0.0500    0.0071\n",
      "    20        0.0759             nan     0.0500    0.0024\n",
      "    40        0.0423             nan     0.0500    0.0005\n",
      "    60        0.0324             nan     0.0500    0.0002\n",
      "    80        0.0281             nan     0.0500    0.0000\n",
      "   100        0.0254             nan     0.0500   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2085             nan     0.1000    0.0306\n",
      "     2        0.1833             nan     0.1000    0.0249\n",
      "     3        0.1625             nan     0.1000    0.0200\n",
      "     4        0.1443             nan     0.1000    0.0174\n",
      "     5        0.1300             nan     0.1000    0.0140\n",
      "     6        0.1177             nan     0.1000    0.0112\n",
      "     7        0.1072             nan     0.1000    0.0100\n",
      "     8        0.0983             nan     0.1000    0.0082\n",
      "     9        0.0914             nan     0.1000    0.0061\n",
      "    10        0.0850             nan     0.1000    0.0058\n",
      "    20        0.0523             nan     0.1000    0.0008\n",
      "    40        0.0383             nan     0.1000    0.0002\n",
      "    60        0.0343             nan     0.1000   -0.0001\n",
      "    80        0.0311             nan     0.1000   -0.0000\n",
      "   100        0.0283             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2073             nan     0.1000    0.0313\n",
      "     2        0.1811             nan     0.1000    0.0245\n",
      "     3        0.1578             nan     0.1000    0.0222\n",
      "     4        0.1386             nan     0.1000    0.0186\n",
      "     5        0.1227             nan     0.1000    0.0144\n",
      "     6        0.1101             nan     0.1000    0.0116\n",
      "     7        0.0989             nan     0.1000    0.0096\n",
      "     8        0.0895             nan     0.1000    0.0084\n",
      "     9        0.0812             nan     0.1000    0.0073\n",
      "    10        0.0745             nan     0.1000    0.0057\n",
      "    20        0.0429             nan     0.1000    0.0007\n",
      "    40        0.0285             nan     0.1000    0.0001\n",
      "    60        0.0237             nan     0.1000    0.0000\n",
      "    80        0.0203             nan     0.1000   -0.0001\n",
      "   100        0.0177             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2243             nan     0.0500    0.0152\n",
      "     2        0.2101             nan     0.0500    0.0137\n",
      "     3        0.1967             nan     0.0500    0.0129\n",
      "     4        0.1846             nan     0.0500    0.0112\n",
      "     5        0.1743             nan     0.0500    0.0098\n",
      "     6        0.1641             nan     0.0500    0.0095\n",
      "     7        0.1551             nan     0.0500    0.0086\n",
      "     8        0.1467             nan     0.0500    0.0079\n",
      "     9        0.1390             nan     0.0500    0.0075\n",
      "    10        0.1324             nan     0.0500    0.0062\n",
      "    20        0.0870             nan     0.0500    0.0027\n",
      "    40        0.0538             nan     0.0500    0.0006\n",
      "    60        0.0437             nan     0.0500    0.0003\n",
      "    80        0.0395             nan     0.0500    0.0001\n",
      "   100        0.0369             nan     0.0500   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2236             nan     0.0500    0.0159\n",
      "     2        0.2083             nan     0.0500    0.0148\n",
      "     3        0.1943             nan     0.0500    0.0135\n",
      "     4        0.1816             nan     0.0500    0.0119\n",
      "     5        0.1697             nan     0.0500    0.0112\n",
      "     6        0.1591             nan     0.0500    0.0098\n",
      "     7        0.1494             nan     0.0500    0.0089\n",
      "     8        0.1404             nan     0.0500    0.0085\n",
      "     9        0.1323             nan     0.0500    0.0074\n",
      "    10        0.1246             nan     0.0500    0.0073\n",
      "    20        0.0774             nan     0.0500    0.0028\n",
      "    40        0.0437             nan     0.0500    0.0008\n",
      "    60        0.0337             nan     0.0500    0.0001\n",
      "    80        0.0289             nan     0.0500   -0.0000\n",
      "   100        0.0259             nan     0.0500   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2090             nan     0.1000    0.0311\n",
      "     2        0.1843             nan     0.1000    0.0236\n",
      "     3        0.1634             nan     0.1000    0.0204\n",
      "     4        0.1451             nan     0.1000    0.0171\n",
      "     5        0.1309             nan     0.1000    0.0138\n",
      "     6        0.1184             nan     0.1000    0.0119\n",
      "     7        0.1093             nan     0.1000    0.0086\n",
      "     8        0.0998             nan     0.1000    0.0087\n",
      "     9        0.0928             nan     0.1000    0.0066\n",
      "    10        0.0862             nan     0.1000    0.0059\n",
      "    20        0.0544             nan     0.1000    0.0010\n",
      "    40        0.0397             nan     0.1000    0.0002\n",
      "    60        0.0349             nan     0.1000   -0.0000\n",
      "    80        0.0316             nan     0.1000   -0.0001\n",
      "   100        0.0291             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2078             nan     0.1000    0.0312\n",
      "     2        0.1805             nan     0.1000    0.0261\n",
      "     3        0.1579             nan     0.1000    0.0219\n",
      "     4        0.1391             nan     0.1000    0.0176\n",
      "     5        0.1237             nan     0.1000    0.0144\n",
      "     6        0.1108             nan     0.1000    0.0118\n",
      "     7        0.0990             nan     0.1000    0.0108\n",
      "     8        0.0899             nan     0.1000    0.0082\n",
      "     9        0.0820             nan     0.1000    0.0068\n",
      "    10        0.0753             nan     0.1000    0.0058\n",
      "    20        0.0431             nan     0.1000    0.0017\n",
      "    40        0.0295             nan     0.1000    0.0000\n",
      "    60        0.0247             nan     0.1000   -0.0001\n",
      "    80        0.0212             nan     0.1000   -0.0001\n",
      "   100        0.0183             nan     0.1000   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2231             nan     0.0500    0.0157\n",
      "     2        0.2083             nan     0.0500    0.0143\n",
      "     3        0.1951             nan     0.0500    0.0127\n",
      "     4        0.1830             nan     0.0500    0.0119\n",
      "     5        0.1723             nan     0.0500    0.0106\n",
      "     6        0.1625             nan     0.0500    0.0095\n",
      "     7        0.1536             nan     0.0500    0.0088\n",
      "     8        0.1454             nan     0.0500    0.0078\n",
      "     9        0.1379             nan     0.0500    0.0072\n",
      "    10        0.1310             nan     0.0500    0.0065\n",
      "    20        0.0850             nan     0.0500    0.0029\n",
      "    40        0.0527             nan     0.0500    0.0006\n",
      "    60        0.0425             nan     0.0500    0.0001\n",
      "    80        0.0384             nan     0.0500    0.0000\n",
      "   100        0.0361             nan     0.0500    0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2224             nan     0.0500    0.0161\n",
      "     2        0.2066             nan     0.0500    0.0151\n",
      "     3        0.1922             nan     0.0500    0.0134\n",
      "     4        0.1795             nan     0.0500    0.0125\n",
      "     5        0.1679             nan     0.0500    0.0110\n",
      "     6        0.1575             nan     0.0500    0.0099\n",
      "     7        0.1485             nan     0.0500    0.0081\n",
      "     8        0.1399             nan     0.0500    0.0079\n",
      "     9        0.1320             nan     0.0500    0.0075\n",
      "    10        0.1246             nan     0.0500    0.0068\n",
      "    20        0.0766             nan     0.0500    0.0028\n",
      "    40        0.0440             nan     0.0500    0.0005\n",
      "    60        0.0338             nan     0.0500    0.0002\n",
      "    80        0.0290             nan     0.0500   -0.0000\n",
      "   100        0.0261             nan     0.0500    0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2078             nan     0.1000    0.0310\n",
      "     2        0.1824             nan     0.1000    0.0237\n",
      "     3        0.1602             nan     0.1000    0.0206\n",
      "     4        0.1438             nan     0.1000    0.0158\n",
      "     5        0.1288             nan     0.1000    0.0138\n",
      "     6        0.1168             nan     0.1000    0.0115\n",
      "     7        0.1071             nan     0.1000    0.0092\n",
      "     8        0.0988             nan     0.1000    0.0077\n",
      "     9        0.0917             nan     0.1000    0.0064\n",
      "    10        0.0848             nan     0.1000    0.0064\n",
      "    20        0.0534             nan     0.1000    0.0019\n",
      "    40        0.0395             nan     0.1000   -0.0000\n",
      "    60        0.0350             nan     0.1000   -0.0000\n",
      "    80        0.0321             nan     0.1000   -0.0000\n",
      "   100        0.0295             nan     0.1000   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2060             nan     0.1000    0.0322\n",
      "     2        0.1789             nan     0.1000    0.0264\n",
      "     3        0.1568             nan     0.1000    0.0212\n",
      "     4        0.1382             nan     0.1000    0.0180\n",
      "     5        0.1229             nan     0.1000    0.0137\n",
      "     6        0.1097             nan     0.1000    0.0121\n",
      "     7        0.0985             nan     0.1000    0.0096\n",
      "     8        0.0898             nan     0.1000    0.0078\n",
      "     9        0.0825             nan     0.1000    0.0062\n",
      "    10        0.0758             nan     0.1000    0.0059\n",
      "    20        0.0448             nan     0.1000    0.0008\n",
      "    40        0.0302             nan     0.1000   -0.0001\n",
      "    60        0.0248             nan     0.1000   -0.0002\n",
      "    80        0.0213             nan     0.1000   -0.0002\n",
      "   100        0.0186             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2247             nan     0.0500    0.0153\n",
      "     2        0.2094             nan     0.0500    0.0143\n",
      "     3        0.1958             nan     0.0500    0.0132\n",
      "     4        0.1836             nan     0.0500    0.0120\n",
      "     5        0.1725             nan     0.0500    0.0104\n",
      "     6        0.1626             nan     0.0500    0.0094\n",
      "     7        0.1533             nan     0.0500    0.0086\n",
      "     8        0.1453             nan     0.0500    0.0075\n",
      "     9        0.1377             nan     0.0500    0.0071\n",
      "    10        0.1308             nan     0.0500    0.0065\n",
      "    20        0.0849             nan     0.0500    0.0027\n",
      "    40        0.0518             nan     0.0500    0.0005\n",
      "    60        0.0420             nan     0.0500    0.0001\n",
      "    80        0.0380             nan     0.0500    0.0000\n",
      "   100        0.0355             nan     0.0500    0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2235             nan     0.0500    0.0160\n",
      "     2        0.2078             nan     0.0500    0.0154\n",
      "     3        0.1938             nan     0.0500    0.0135\n",
      "     4        0.1809             nan     0.0500    0.0121\n",
      "     5        0.1688             nan     0.0500    0.0113\n",
      "     6        0.1581             nan     0.0500    0.0101\n",
      "     7        0.1484             nan     0.0500    0.0091\n",
      "     8        0.1392             nan     0.0500    0.0087\n",
      "     9        0.1310             nan     0.0500    0.0076\n",
      "    10        0.1238             nan     0.0500    0.0067\n",
      "    20        0.0760             nan     0.0500    0.0025\n",
      "    40        0.0431             nan     0.0500    0.0007\n",
      "    60        0.0327             nan     0.0500    0.0001\n",
      "    80        0.0282             nan     0.0500    0.0001\n",
      "   100        0.0254             nan     0.0500   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2097             nan     0.1000    0.0295\n",
      "     2        0.1832             nan     0.1000    0.0254\n",
      "     3        0.1613             nan     0.1000    0.0212\n",
      "     4        0.1439             nan     0.1000    0.0170\n",
      "     5        0.1291             nan     0.1000    0.0147\n",
      "     6        0.1166             nan     0.1000    0.0113\n",
      "     7        0.1065             nan     0.1000    0.0094\n",
      "     8        0.0975             nan     0.1000    0.0082\n",
      "     9        0.0904             nan     0.1000    0.0066\n",
      "    10        0.0837             nan     0.1000    0.0060\n",
      "    20        0.0525             nan     0.1000    0.0010\n",
      "    40        0.0388             nan     0.1000    0.0001\n",
      "    60        0.0342             nan     0.1000   -0.0001\n",
      "    80        0.0308             nan     0.1000   -0.0000\n",
      "   100        0.0282             nan     0.1000   -0.0000\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2070             nan     0.1000    0.0323\n",
      "     2        0.1799             nan     0.1000    0.0260\n",
      "     3        0.1578             nan     0.1000    0.0205\n",
      "     4        0.1388             nan     0.1000    0.0178\n",
      "     5        0.1224             nan     0.1000    0.0156\n",
      "     6        0.1096             nan     0.1000    0.0112\n",
      "     7        0.0988             nan     0.1000    0.0098\n",
      "     8        0.0893             nan     0.1000    0.0084\n",
      "     9        0.0813             nan     0.1000    0.0071\n",
      "    10        0.0749             nan     0.1000    0.0054\n",
      "    20        0.0441             nan     0.1000    0.0011\n",
      "    40        0.0292             nan     0.1000   -0.0001\n",
      "    60        0.0244             nan     0.1000   -0.0002\n",
      "    80        0.0208             nan     0.1000   -0.0001\n",
      "   100        0.0179             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        0.2076             nan     0.1000    0.0319\n",
      "     2        0.1794             nan     0.1000    0.0271\n",
      "     3        0.1572             nan     0.1000    0.0211\n",
      "     4        0.1384             nan     0.1000    0.0181\n",
      "     5        0.1228             nan     0.1000    0.0143\n",
      "     6        0.1095             nan     0.1000    0.0123\n",
      "     7        0.0979             nan     0.1000    0.0110\n",
      "     8        0.0889             nan     0.1000    0.0082\n",
      "     9        0.0817             nan     0.1000    0.0063\n",
      "    10        0.0750             nan     0.1000    0.0058\n",
      "    20        0.0445             nan     0.1000    0.0011\n",
      "    40        0.0302             nan     0.1000   -0.0000\n",
      "    60        0.0253             nan     0.1000   -0.0000\n",
      "    80        0.0218             nan     0.1000   -0.0001\n",
      "   100        0.0191             nan     0.1000   -0.0001\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Stochastic Gradient Boosting \n",
       "\n",
       "3221 samples\n",
       "  57 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 2899, 2899, 2899, 2899, 2899, 2899, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  shrinkage  interaction.depth  n.trees  RMSE       Rsquared   MAE      \n",
       "  0.05       10                  30      0.2668386  0.7527380  0.2116533\n",
       "  0.05       10                  50      0.2358187  0.7763205  0.1471022\n",
       "  0.05       10                 100      0.2201078  0.7973148  0.1224115\n",
       "  0.05       20                  30      0.2567429  0.7705557  0.2001979\n",
       "  0.05       20                  50      0.2268419  0.7927097  0.1381438\n",
       "  0.05       20                 100      0.2123215  0.8111902  0.1153843\n",
       "  0.10       10                  30      0.2308089  0.7799621  0.1317488\n",
       "  0.10       10                  50      0.2229583  0.7917263  0.1254799\n",
       "  0.10       10                 100      0.2175175  0.8012874  0.1261398\n",
       "  0.10       20                  30      0.2212990  0.7973110  0.1230020\n",
       "  0.10       20                  50      0.2145248  0.8071628  0.1182559\n",
       "  0.10       20                 100      0.2112249  0.8127247  0.1204714\n",
       "\n",
       "Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were n.trees = 100, interaction.depth =\n",
       " 20, shrinkage = 0.1 and n.minobsinnode = 10."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data1_sgb_model = train(V58 ~ ., data = data1_train_1, method = \"gbm\", trControl = fitControl, tuneGrid = sgb_grid)\n",
    "data1_sgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>alpha</th><th scope=col>lambda</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " alpha & lambda\\\\\n",
       "\\hline\n",
       "\t 1 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| alpha | lambda |\n",
       "|---|---|\n",
       "| 1 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  alpha lambda\n",
       "1 1     0     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>cp</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.01</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " cp\\\\\n",
       "\\hline\n",
       "\t 0.01\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| cp |\n",
       "|---|\n",
       "| 0.01 |\n",
       "\n"
      ],
      "text/plain": [
       "  cp  \n",
       "1 0.01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>mtry</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>3</th><td>7</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & mtry\\\\\n",
       "\\hline\n",
       "\t3 & 7\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | mtry |\n",
       "|---|---|\n",
       "| 3 | 7 |\n",
       "\n"
      ],
      "text/plain": [
       "  mtry\n",
       "3 7   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>n.trees</th><th scope=col>interaction.depth</th><th scope=col>shrinkage</th><th scope=col>n.minobsinnode</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>12</th><td>100</td><td>20 </td><td>0.1</td><td>10 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & n.trees & interaction.depth & shrinkage & n.minobsinnode\\\\\n",
       "\\hline\n",
       "\t12 & 100 & 20  & 0.1 & 10 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | n.trees | interaction.depth | shrinkage | n.minobsinnode |\n",
       "|---|---|---|---|---|\n",
       "| 12 | 100 | 20  | 0.1 | 10  |\n",
       "\n"
      ],
      "text/plain": [
       "   n.trees interaction.depth shrinkage n.minobsinnode\n",
       "12 100     20                0.1       10            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data1_glm_model$bestTune\n",
    "data1_dt_model$bestTune\n",
    "data1_rf_model$bestTune\n",
    "data1_sgb_model$bestTune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters for 4 models are shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>glm_RMSE</th><th scope=col>dt_RMSE</th><th scope=col>rf_RMSE</th><th scope=col>sgb_RMSE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.3439133</td><td>0.2841334</td><td>0.2087809</td><td>0.2112249</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " glm\\_RMSE & dt\\_RMSE & rf\\_RMSE & sgb\\_RMSE\\\\\n",
       "\\hline\n",
       "\t 0.3439133 & 0.2841334 & 0.2087809 & 0.2112249\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| glm_RMSE | dt_RMSE | rf_RMSE | sgb_RMSE |\n",
       "|---|---|---|---|\n",
       "| 0.3439133 | 0.2841334 | 0.2087809 | 0.2112249 |\n",
       "\n"
      ],
      "text/plain": [
       "  glm_RMSE  dt_RMSE   rf_RMSE   sgb_RMSE \n",
       "1 0.3439133 0.2841334 0.2087809 0.2112249"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm_RMSE = min(data1_glm_model$result$RMSE)\n",
    "dt_RMSE = min(data1_dt_model$result$RMSE)\n",
    "rf_RMSE = min(data1_rf_model$result$RMSE)\n",
    "sgb_RMSE = min(data1_sgb_model$result$RMSE)\n",
    "data.table(glm_RMSE,dt_RMSE,rf_RMSE,sgb_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>glm_MAE</th><th scope=col>dt_MAE</th><th scope=col>rf_MAE</th><th scope=col>sgb_MAE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.2693482</td><td>0.1561977</td><td>0.1201925</td><td>0.1153843</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " glm\\_MAE & dt\\_MAE & rf\\_MAE & sgb\\_MAE\\\\\n",
       "\\hline\n",
       "\t 0.2693482 & 0.1561977 & 0.1201925 & 0.1153843\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| glm_MAE | dt_MAE | rf_MAE | sgb_MAE |\n",
       "|---|---|---|---|\n",
       "| 0.2693482 | 0.1561977 | 0.1201925 | 0.1153843 |\n",
       "\n"
      ],
      "text/plain": [
       "  glm_MAE   dt_MAE    rf_MAE    sgb_MAE  \n",
       "1 0.2693482 0.1561977 0.1201925 0.1153843"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm_MAE = min(data1_glm_model$result$MAE)\n",
    "dt_MAE = min(data1_dt_model$result$MAE)\n",
    "rf_MAE = min(data1_rf_model$result$MAE)\n",
    "sgb_MAE = min(data1_sgb_model$result$MAE)\n",
    "data.table(glm_MAE,dt_MAE,rf_MAE,sgb_MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen the results above, SGB and RF models have the best MAE and RMSE results. RF'S RMSE is better than SGB and SGB's MAE is better than RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_glm_predict <- predict(data1_glm_model,data1_test_1)   \n",
    "data1_dt_predict <- predict(data1_dt_model,data1_test_1)   \n",
    "data1_rf_predict <- predict(data1_rf_model,data1_test_1)   \n",
    "data1_sgb_predict <- predict(data1_sgb_model,data1_test_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>glm_Test_RMSE</th><th scope=col>dt_Test_RMSE</th><th scope=col>rf_Test_RMSE</th><th scope=col>sgb_Test_RMSE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.3359062</td><td>0.2773426</td><td>0.2005818</td><td>0.2075044</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " glm\\_Test\\_RMSE & dt\\_Test\\_RMSE & rf\\_Test\\_RMSE & sgb\\_Test\\_RMSE\\\\\n",
       "\\hline\n",
       "\t 0.3359062 & 0.2773426 & 0.2005818 & 0.2075044\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| glm_Test_RMSE | dt_Test_RMSE | rf_Test_RMSE | sgb_Test_RMSE |\n",
       "|---|---|---|---|\n",
       "| 0.3359062 | 0.2773426 | 0.2005818 | 0.2075044 |\n",
       "\n"
      ],
      "text/plain": [
       "  glm_Test_RMSE dt_Test_RMSE rf_Test_RMSE sgb_Test_RMSE\n",
       "1 0.3359062     0.2773426    0.2005818    0.2075044    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm_Test_RMSE = RMSE(data1_test_1$V58,data1_glm_predict)\n",
    "dt_Test_RMSE = RMSE(data1_test_1$V58,data1_dt_predict)\n",
    "rf_Test_RMSE = RMSE(data1_test_1$V58,data1_rf_predict)\n",
    "sgb_Test_RMSE = RMSE(data1_test_1$V58,data1_sgb_predict)\n",
    "RSME_result = data.table(glm_Test_RMSE,dt_Test_RMSE,rf_Test_RMSE,sgb_Test_RMSE)\n",
    "RSME_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, RF's RMSE result is almost same as SGB. Therefore RF with MTRY=7 or SGB with tree=100, interaction depht = 20, shrinkage = 0.1 and minobsinnode = 10 can be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
